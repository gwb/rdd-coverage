{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Motivation</a></div><div class=\"lev2 toc-item\"><a href=\"#Prior-attempts\" data-toc-modified-id=\"Prior-attempts-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prior attempts</a></div><div class=\"lev1 toc-item\"><a href=\"#Model-Specification\" data-toc-modified-id=\"Model-Specification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model Specification</a></div><div class=\"lev2 toc-item\"><a href=\"#Notation\" data-toc-modified-id=\"Notation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Notation</a></div><div class=\"lev2 toc-item\"><a href=\"#1GP-solution\" data-toc-modified-id=\"1GP-solution-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>1GP solution</a></div><div class=\"lev2 toc-item\"><a href=\"#2GP-solution\" data-toc-modified-id=\"2GP-solution-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>2GP solution</a></div><div class=\"lev2 toc-item\"><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Discussion</a></div><div class=\"lev1 toc-item\"><a href=\"#Inference\" data-toc-modified-id=\"Inference-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Inference</a></div><div class=\"lev2 toc-item\"><a href=\"#1GP\" data-toc-modified-id=\"1GP-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>1GP</a></div><div class=\"lev2 toc-item\"><a href=\"#2GP\" data-toc-modified-id=\"2GP-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>2GP</a></div><div class=\"lev1 toc-item\"><a href=\"#Handling-covariates\" data-toc-modified-id=\"Handling-covariates-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Handling covariates</a></div><div class=\"lev1 toc-item\"><a href=\"#Average-treatment-effect\" data-toc-modified-id=\"Average-treatment-effect-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Average treatment effect</a></div><div class=\"lev1 toc-item\"><a href=\"#2GP:-Testing-for-non-zero-effect\" data-toc-modified-id=\"2GP:-Testing-for-non-zero-effect-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>2GP: Testing for non-zero effect</a></div><div class=\"lev2 toc-item\"><a href=\"#Using-the-inverse-variance-weighted-mean-treatment-effect-posterior-to-test-the-weak-null-hypothesis\" data-toc-modified-id=\"Using-the-inverse-variance-weighted-mean-treatment-effect-posterior-to-test-the-weak-null-hypothesis-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Using the inverse-variance weighted mean treatment effect posterior to test the weak null hypothesis</a></div><div class=\"lev2 toc-item\"><a href=\"#Likelihood-based-sharp-null-test\" data-toc-modified-id=\"Likelihood-based-sharp-null-test-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Likelihood-based sharp null test</a></div><div class=\"lev2 toc-item\"><a href=\"#$\\chi^2$-test-for-the-sharp-null\" data-toc-modified-id=\"$\\chi^2$-test-for-the-sharp-null-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03C7;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-3152\" style=\"width: 1.144em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.033em; height: 0px; font-size: 108%;\"><span style=\"position: absolute; clip: rect(1.107em, 1001.033em, 2.396em, -999.998em); top: -2.061em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-3153\"><span class=\"msubsup\" id=\"MathJax-Span-3154\"><span style=\"display: inline-block; position: relative; width: 0.997em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.465em, 1000.518em, 4.349em, -999.998em); top: -4.014em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-3155\" style=\"font-family: STIXMathJax_Main-italic;\">χ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.039em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.017em;\"></span></span><span style=\"position: absolute; top: -4.382em; left: 0.554em;\"><span class=\"mn\" id=\"MathJax-Span-3156\" style=\"font-size: 70.7%; font-family: STIXMathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.017em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.065em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.277em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.196em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>χ</mi><mn>2</mn></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">\\chi^2</script> test for the sharp null</a></div><div class=\"lev2 toc-item\"><a href=\"#Power-in-simulated-example\" data-toc-modified-id=\"Power-in-simulated-example-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Power in simulated example</a></div><div class=\"lev2 toc-item\"><a href=\"#Placebo-tests\" data-toc-modified-id=\"Placebo-tests-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Placebo tests</a></div><div class=\"lev1 toc-item\"><a href=\"#Spatial-advantage\" data-toc-modified-id=\"Spatial-advantage-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Spatial advantage</a></div><div class=\"lev1 toc-item\"><a href=\"#Example:-NYC-school-districts\" data-toc-modified-id=\"Example:-NYC-school-districts-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Example: NYC school districts</a></div><div class=\"lev2 toc-item\"><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Exploratory-analysis\" data-toc-modified-id=\"Exploratory-analysis-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Exploratory analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#Model-for-property-prices\" data-toc-modified-id=\"Model-for-property-prices-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Model for property prices</a></div><div class=\"lev2 toc-item\"><a href=\"#parameter-optimization\" data-toc-modified-id=\"parameter-optimization-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>parameter optimization</a></div><div class=\"lev2 toc-item\"><a href=\"#cliff-face\" data-toc-modified-id=\"cliff-face-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>cliff face</a></div><div class=\"lev3 toc-item\"><a href=\"#surface-plots\" data-toc-modified-id=\"surface-plots-8.5.1\"><span class=\"toc-item-num\">8.5.1&nbsp;&nbsp;</span>surface plots</a></div><div class=\"lev2 toc-item\"><a href=\"#average-effect\" data-toc-modified-id=\"average-effect-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>average effect</a></div><div class=\"lev2 toc-item\"><a href=\"#significance-tests\" data-toc-modified-id=\"significance-tests-8.7\"><span class=\"toc-item-num\">8.7&nbsp;&nbsp;</span>significance tests</a></div><div class=\"lev3 toc-item\"><a href=\"#placebo-tests\" data-toc-modified-id=\"placebo-tests-8.7.1\"><span class=\"toc-item-num\">8.7.1&nbsp;&nbsp;</span>placebo tests</a></div><div class=\"lev2 toc-item\"><a href=\"#pairwise-treatment-effect-(all-districts)\" data-toc-modified-id=\"pairwise-treatment-effect-(all-districts)-8.8\"><span class=\"toc-item-num\">8.8&nbsp;&nbsp;</span>pairwise treatment effect (all districts)</a></div><div class=\"lev1 toc-item\"><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\newcommand{\\genericdel}[3]{%\n",
    "      \\left#1#3\\right#2\n",
    "    }\n",
    "    \\newcommand{\\del}[1]{\\genericdel(){#1}}\n",
    "    \\newcommand{\\sbr}[1]{\\genericdel[]{#1}}\n",
    "    \\newcommand{\\cbr}[1]{\\genericdel\\{\\}{#1}}\n",
    "    \\newcommand{\\abs}[1]{\\genericdel||{#1}}\n",
    "    \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "    \\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "    \\DeclareMathOperator{\\Pr}{\\mathbb{p}}\n",
    "    \\DeclareMathOperator{\\E}{\\mathbb{E}}\n",
    "    \\DeclareMathOperator{\\Ind}{\\mathbb{I}}\n",
    "    \\DeclareMathOperator{\\V}{\\mathbb{V}}\n",
    "    \\DeclareMathOperator{\\cov}{Cov}\n",
    "    \\DeclareMathOperator{\\var}{Var}\n",
    "    \\DeclareMathOperator{\\ones}{\\mathbf{1}}\n",
    "    \\DeclareMathOperator{\\invchi}{\\mathrm{Inv-\\chi}^2}\n",
    "    \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "    \\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "    \\newcommand{\\effect}{\\mathrm{eff}}\n",
    "    \\newcommand{\\xtilde}{\\widetilde{X}}\n",
    "    \\DeclareMathOperator{\\normal}{\\mathcal{N}}\n",
    "    \\DeclareMathOperator{\\unif}{Uniform}\n",
    "    \\newcommand{\\boxleft}{\\unicode{x25E7}}\n",
    "    \\newcommand{\\boxright}{\\unicode{x25E8}}\n",
    "    \\newcommand{\\discont}{\\unicode{x25EB}}\n",
    "    \\newcommand{\\jleft}{\\unicode{x21E5}}\n",
    "    \\newcommand{\\jright}{\\unicode{x21E4}}\n",
    "    \\DeclareMathOperator*{\\gp}{\\mathcal{GP}}\n",
    "    \\newcommand{\\trans}{^{\\intercal}}\n",
    "    \\newcommand{\\scrS}{\\mathscr{S}}\n",
    "    \\newcommand{\\sigmaf}{\\sigma_{\\mathrm{GP}}}\n",
    "    \\newcommand{\\sigman}{\\sigma_{\\epsilon}}\n",
    "    \\newcommand{\\sigmatau}{\\sigma_{\\tau}}\n",
    "    \\newcommand{\\sigmabeta}{\\sigma_{\\beta}}\n",
    "    \\newcommand{\\sigmamu}{\\sigma_{\\mu}}\n",
    "    \\newcommand{\\sigmagamma}{\\sigma_{\\gamma}}\n",
    "    \\newcommand{\\svec}{\\mathbf{s}}\n",
    "    \\newcommand{\\yvec}{\\mathbf{y}}\n",
    "    \\newcommand{\\muvec}{\\mathbf{\\mu}}\n",
    "    \\newcommand{\\indep}{\\perp}\n",
    "    \\newcommand{\\iid}{iid}\n",
    "    \\newcommand{\\vectreat}{\\Ind_{T}}\n",
    "    \\newcommand{\\yt}{Y^\\mathrm{T}}\n",
    "    \\newcommand{\\yc}{Y^\\mathrm{C}}\n",
    "    \\newcommand{\\boundary}{\\partial}\n",
    "    \\newcommand{\\sentinels}{\\mathbf{\\boundary}}\n",
    "    \\newcommand{\\eye}{\\mathbf{I}}\n",
    "    \\newcommand{\\K}{\\mathbf{K}}\n",
    "    \\DeclareMathOperator{\\trace}{trace}\n",
    "    \\newcommand{\\linavg}{\\bar{\\tau}}\n",
    "    \\newcommand{\\invvar}{\\tau^{IV}}\n",
    "    \\newcommand{\\modnull}{\\mathscr{M}_0}\n",
    "    \\newcommand{\\modalt}{\\mathscr{M}_1}\n",
    "    \\newcommand{\\degree}{\\hspace{0pt}^\\circ}\n",
    "    % NYC %\n",
    "    \\newcommand{\\saleprice}{\\mathtt{SalePrice}}\n",
    "    \\newcommand{\\sqft}{\\mathtt{SQFT}}\n",
    "    \\newcommand{\\xvec}{\\mathbf{x}}\n",
    "    \\newcommand{\\tax}{\\mathtt{TaxClass}}\n",
    "    \\newcommand{\\building}{\\mathtt{BuildingClass}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using LaTeXStrings\n",
    "using GaussianProcesses\n",
    "using Distributions\n",
    "using Base.LinAlg\n",
    "using Distances\n",
    "import PyPlot; plt=PyPlot\n",
    "plt.rc(\"figure\", dpi=300.0)\n",
    "# plt.rc(\"figure\", figsize=(6,4))\n",
    "plt.rc(\"figure\", autolayout=true)\n",
    "plt.rc(\"savefig\", dpi=300.0)\n",
    "plt.rc(\"text\", usetex=true)\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "plt.rc(\"font\", serif=\"Palatino\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Motivation\n",
    "\n",
    "## Prior attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specification\n",
    "\n",
    "## Notation\n",
    "\n",
    "* 2-dimensional coordinate space $\\scrS$\n",
    "* treatment units are in region $\\scrS_T \\subset \\scrS$ and control units are in non-overlapping $\\scrS_C$ outside of the treatment region, so that $\\scrS_C = \\scrS_T^c$ and $\\scrS_T \\cup \\scrS_C = \\scrS$\n",
    "* Observed outcomes for units in treatment region $s \\in \\scrS_T$ are labeled $Y_T(\\svec)$, and units in control region $Y_C(\\svec)$.\n",
    "* Potential outcomes framework: Each unit has a potential outcome under treatment $Y_T(\\svec)$ and a potential outcome under control $Y_C(\\svec)$. If $s \\in \\scrS_T$, then $Y_T(\\svec)$ is observed, otherwise $Y_C(\\svec)$ is observed.\n",
    "\n",
    "\n",
    "## 1GP solution\n",
    "\n",
    "Most straightforwardly, we model the observed outcomes $Y$ at locations $S$ as the sum of an intercept $\\mu$, linear trend $S\\beta$, a spatial Gaussian process $f(S)$, a constant treatment effect $\\tau$ in the treatment region, and iid normal noise $\\epsilon$.\n",
    "\n",
    "\\begin{equation}\\begin{split}\n",
    "Y_i(\\svec) &= \\mu+\\svec\\trans\\beta + f(\\svec) + \\tau \\Ind\\cbr{\\svec \\in \\scrS_T} + \\epsilon_i \\\\\n",
    "f(S) &\\sim \\gp\\del{0, k(\\svec, \\svec')} \\\\\n",
    "k(\\svec,\\svec') &= \\sigmaf^2 \\exp\\del{ - \\frac{\\del{\\svec-\\svec'}\\trans\\del{\\svec-\\svec'}}{2 \\ell^2}} \\\\\n",
    "\\epsilon_i &\\overset{\\iid}{\\sim} \\normal\\del{0,\\sigma_\\epsilon^2}\n",
    "\\end{split}\\end{equation}\n",
    "$f(S)$ is a smooth surface covering all of $\\scrS$, specificed as a Gaussian Process with squared exponential covariance kernel $k$ with lengthscale $\\ell$ and variance $\\sigmaf^2$. The squared exponential kernel is frequently used in spatial settings. The constant treatment effect implies the assumption that $Y_T(\\svec) = \\tau + Y_C(\\svec)$ for all units at all locations.\n",
    "\n",
    "## 2GP solution\n",
    "\n",
    "The constant treatment effect is a strong assumption that will be hard to justify in many applications. To allow the treatment effect to vary spatially, an alternative is to specify two independent Gaussian processes for the treatment response and the control response.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Y_{T,i}(\\svec) &= \\underbrace{\\mu_T + \\svec\\trans\\beta_T + f_T(\\svec)}_{g_T(\\svec)} + \\epsilon_i \\\\\n",
    "Y_{C,i}(\\svec) &= \\underbrace{\\mu_C + \\svec\\trans\\beta_C + f_C(\\svec)}_{g_C(\\svec)} + \\epsilon_i \\\\\n",
    "f_T(S), f_C(S) &\\overset{\\indep}{\\sim} \\gp\\del{0, k(\\svec, \\svec')} \\\\\n",
    "k(\\svec,\\svec') &= \\sigmaf^2 \\exp\\del{ - \\frac{\\del{\\svec-\\svec'}\\trans\\del{\\svec-\\svec'}}{2 \\ell^2}}\n",
    "\\end{split}\n",
    "\\label{eq:spec2gp}\n",
    "\\end{equation}\n",
    "\n",
    "Here, the treatment effect $\\tau$ is no longer included explicitly in the model. Instead, the treatment effect at a location $\\svec$ is derived as the difference between the two (noise-free) surfaces $g_T$ and $g_C$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\tau(\\svec) = \\sbr{\\mu_T + \\svec\\trans\\beta_T + f_T(\\svec)} - \\sbr{\\mu_C + \\svec\\trans\\beta_C + f_C(\\svec)}\n",
    "\\end{equation}\n",
    "\n",
    "In this specification, the kernel parameters $\\ell$ and $\\sigmaf$ are the same in the treatment and control regions, so we assume that the spatial smoothness of the responses isn't affected by the treatment. This assumption will be reasonable in most applications, but can be easily relaxed. Inference on the hyperparameters proceeds as in the 1GP case, using the sum of the likelihood in the treatment and control regions.\n",
    "\n",
    "## Discussion\n",
    "* different assumptions\n",
    "* will stick to 2GP from now on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "By specifying the spatial variation as Gaussian processes, we can leverage the properties of multivariate normals to obtain analytical forms for the estimate of the treatment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1GP\n",
    "\n",
    "We proceed by placing normal priors on $\\mu$, $\\beta$ and $\\tau$. The model specification can then be used to obtain covariances between the observations and these parameters. In fact, $\\del{Y,f(S),\\tau,\\mu,\\beta} \\mid \\ell,\\sigmaf$ is multi-variate normal with variance-covariance given by\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\tau  &\\sim \\normal\\del{0,\\sigmatau^2} \\\\\n",
    "    \\mu   &\\sim \\normal\\del{0,\\sigmamu^2} \\\\\n",
    "    \\beta &\\sim \\normal\\del{0,\\sigmabeta^2} \\\\\n",
    "    \\cov(Y_i(\\svec),\\tau) &= \\sigmatau^2 \\Ind\\cbr{\\svec \\in \\scrS_T} \\\\\n",
    "    \\cov(Y_i(\\svec),\\mu)  &= \\sigmamu^2 \\\\\n",
    "    \\cov(Y_i(\\svec),\\beta)&= \\sigmabeta^2 \\svec\\trans \\svec \\\\\n",
    "    \\cov(Y_i(\\svec),Y_i(\\svec'))&= \\sigmamu^2 + \\sigmatau^2 \\Ind\\cbr{\\svec \\in \\scrS_T}\\Ind\\cbr{\\svec' \\in \\scrS_T} + \\sigmabeta^2 \\svec\\trans \\svec' + k(\\svec,\\svec') + \\delta_{ij}\\sigman^2\\\\\n",
    "    \\cov(Y(\\svec),f(\\svec')) = \\cov(f(\\svec),f(\\svec')) &= k(\\svec,\\svec')\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Multi-variate theory then allows us to condition any of these objects on the others. We are particularly interested in the posterior distribution $\\tau \\mid Y, \\ell, \\sigmaf$ which is given by\n",
    "\\begin{equation}\n",
    "    \\tau \\mid Y, \\ell, \\sigmaf \\sim \\normal\\del{\\cov\\del{Y,\\tau}\\trans \\cov\\del{Y}^{-1} Y, \\sigma_\\tau^2 - \\cov\\del{Y,\\tau}\\trans \\cov\\del{Y}^{-1} \\cov\\del{Y,\\tau}}\n",
    "\\end{equation}\n",
    "\n",
    "To proceed computationally, we define the treatment indicator vector $\\vectreat$ with $i$th entry equal to 0 when $\\svec_i$ is in the control region, and 1 in the treatment region, and the $n \\times n$ kernel covariance matrix $\\K$ having entries $\\K_{ij}=k(\\svec_i, \\svec_j)$. The posterior mean and variance are then easily computed.\n",
    "\n",
    "\\begin{equation}\\begin{split}\n",
    "    \\E \\del{\\tau \\mid Y, \\ell, \\sigmaf, \\sigman} &= \\sigmatau^2 \\vectreat\\trans \\cbr{\\sigmamu^2 + \\sigmatau^2 \\vectreat \\vectreat\\trans + \\sigmabeta^2 S S\\trans + \\K + \\sigman^2 \\eye }^{-1} Y \\\\\n",
    "    \\var \\del{\\tau \\mid Y, \\ell, \\sigmaf, \\sigman} &= \\sigma_\\tau^2 - \\sigma_\\tau^2 \\vectreat\\trans \\cbr{\\sigmamu^2 + \\sigmatau^2 \\vectreat \\vectreat\\trans + \\sigmabeta^2 S S\\trans + \\K + \\sigman^2 \\eye }^{-1} \\vectreat\n",
    "\\end{split}\\end{equation}\n",
    "\n",
    "What remains is the inference on the hyperparameters $\\sigman, \\sigmaf$ and $\\ell$. The two approaches typically taken in modern spatial statistics are either to maximize the marginal likelihood of $Y$ as a function of those three parameters, or to assign them a prior and take a Bayesian approach, requiring that the posterior of $\\tau$ be integrated over those parameters. The compromise is clear: the Bayesian approach incorporates the uncertainty in the hyperparameters, thus giving more reliable inference on $\\tau$, but maximizing the marginal likelihood has a much lower computation cost. Therefore, we recommend taking the Bayesian approach whenever computationally possible, and maximizing the marginal likelihood when the data is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2GP\n",
    "\n",
    "In the 2GP setting, we begin by modeling the treatment and control units with two independent Gaussian processes with shared hyperparameters. Because the treatment and control regions do not overlap, inference on the treatment effect is only measurable near the boundary. In the classical one-dimensional regression discontinuity design, the estimand is therefore defined at the boundary $x=b$: \n",
    "\\begin{equation}\n",
    "\\tau = \\lim_{x \\downarrow b} \\E\\sbr{y \\mid X=s} - \\lim_{x \\uparrow b} \\E\\sbr{y \\mid X=x} = \\E\\sbr{Y_T \\mid X=b} - \\E\\sbr{Y_C \\mid X=b}\n",
    "\\end{equation}\n",
    "Analogously, we focus on the treatment effect at the boundary $\\boundary$ between the treatment and control regions. $\\boundary$ is therefore a one-dimensional subset of $\\scrS$. We will proceed by extrapolating both Gaussian processes to the boundary, and then subtracting the predictions to obtain the estimated treatment effect. Computationally, we need to represent this boundary as a set of $k$ “sentinel” units distributed along the boundary $\\sentinels=\\cbr{\\boundary_1,\\ldots,\\boundary_k},~\\partial_i \\in \\partial$. The extrapolation step then proceeds mechanically through multivariate-normal theory.\n",
    "\\begin{equation}\\begin{split}\n",
    "    g_T(\\sentinels) \\mid Y_T, S_T, \\ell, \\sigmaf, \\sigman &\\sim \\normal\\del{\\mu_{\\sentinels \\mid T}, \\Sigma_{\\sentinels \\mid T}} \\\\\n",
    "    \\mu_{\\sentinels \\mid T} &\\equiv \\cov\\del{g_T(\\sentinels), Y_T} \\cov\\del{Y_T}^{-1}  Y_T \\\\\n",
    "    \\Sigma_{\\sentinels \\mid T} &\\equiv \\cov \\del{g_T(\\sentinels)} - \\cov\\del{g_T(\\sentinels), Y_T} \\cov\\del{Y_T}^{-1} \\cov\\del{Y_T,g_T(\\sentinels) \\label{eq:postvarT2gp}\n",
    "    }\n",
    "\\end{split}\\end{equation}\n",
    "All the covariance terms can be derived from the model similarly to what we saw in the 1GP procedure. Analogously, we also generate predictions for $g_C(\\sentinels)$ using the data in the control region, and denote their posterior mean and covariance as $\\mu_{\\sentinels \\mid C}$ and $\\Sigma_{\\sentinels \\mid C}$. Since the two surfaces are modeled as independent, the treatment effect $\\tau(\\sentinels)=g_T(\\sentinels)-g_C(\\sentinels)$ along the boundary is also multivariate normal with posterior mean and covariance\n",
    "\\begin{equation}\\begin{split}\n",
    "    \\mu_{\\boundary \\mid Y} &= \\E \\del{\\tau(\\sentinels) \\mid Y_T, Y_C} = \\mu_{\\sentinels \\mid T} - \\mu_{\\sentinels \\mid C} \\\\\n",
    "    \\Sigma_{\\boundary \\mid Y} &= \\cov\\del{\\tau(\\sentinels) \\mid Y_T, Y_C} = \\Sigma_{\\sentinels \\mid T} + \\Sigma_{\\sentinels \\mid C}\\,.\n",
    "\\end{split}\\label{eq:postvar2gp}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling covariates\n",
    "\n",
    "The Gaussian Process specification makes it easy to incorporate a linear model on non-spatial covariates, both mathematically and computationally. The model is modified by the addition of the linear regression term $D \\gamma$ on the $n \\times p$ matrix of covariates $D$. In the spirit of ridge regression, we recommend placing a normal prior $\\normal(0,\\sigmagamma^2)$ on the regression coefficients. This preserves the multivariate normality of the problem, with the simple addition of a term $\\sigmagamma^2 D\\trans D$ to the covariance of $Y$. \n",
    "\n",
    "With the 1GP model, covariates can therefore be handled at very little additional cost, except that the additional hyperparameter $\\sigmagamma^2$ needs to be fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average treatment effect\n",
    "\n",
    "Once we obtain the posterior on the treatment effect function $\\tau(\\boundary)$, estimating the average treatment effect along the boundary will often be of interest. Most straightforwardly, if the sentinels are evenly spaced, we can estimate $\\linavg$, the mean of $\\tau(\\svec)$ along the boundary, by averaging the entries of the mean posterior at the sentinels. If the sentinels are not evenly spaced, then each entry needs to be re-weighted by the length of the border that the sentinel occupies.\n",
    "\\begin{equation}\\begin{split}\n",
    "    \\linavg &\\equiv \\frac{\\oint_\\boundary \\left. \\tau(x) dx \\right.}{\\oint_\\boundary \\left. dx \\right.} \\\\\n",
    "    \\linavg \\mid Y_T, Y_C, \\sigmaf, \\sigman, \\ell &\\sim \\normal\\del{\\mu_{\\linavg \\mid Y}, \\Sigma_{\\linavg \\mid Y}} \\\\\n",
    "    \\mu_{\\linavg \\mid Y} &\\approx \\del{\\ones\\trans \\mu_{\\sentinels \\mid Y}} / n_{\\sentinels} \\\\\n",
    "    \\Sigma_{\\linavg \\mid Y} &\\approx \\del{\\ones\\trans \\Sigma_{\\sentinels \\mid Y} \\ones} / n_{\\sentinels}^2\n",
    "\\end{split}\\end{equation}\n",
    "This procedure is mathematically sound, but the choice of the $\\linavg$ estimand raises two problems. Firstly, parts of the border adjoining dense populations are given equal weights to those in sparsely populated areas. If the border goes through an unpopulated area, like a lake or a public park, then the treatment effect there has little meaning and importance. Furthermore, $\\tau(\\svec)$ in those areas will have large posterior variances, which will dominate the posterior variance of $\\linavg$, making otherwise large treatment effects difficult to detect.\n",
    "\n",
    "![mississippi counts](figures/mississippi_counts.png)\n",
    "\n",
    "Secondly, the unweighted mean treatment estimand is affected by the shape of the border between the treatment and control regions.\n",
    "We illustrate this with the border separating two American States: Louisiana and Mississippi.\n",
    "From North to South, the border follows the meandering Mississippi river, then takes a sharp turn to the East and becomes a straight line, until it meets the even more sinuous Pearl river, which it then follows until it reaches the Gulf of Mexico. \n",
    "Sentinels placed at constant intervals along this interval will therefore be most densely packed along the Pearl River, and sparsest along the straight segment of the border (see Figure \\ref{fig::mississippi_counts}).\n",
    "When averaging a function over the border, those sections will therefore be overrepresented. \n",
    "Troublingly, the sinuousness of the border therefore determines the estimand, and the resolution of our map can drastically change our estimate, even though the outcomes of the treatment we are studying might have nothing to do with river topographies.\n",
    "\n",
    "Weighing the treatment effect at each sentinel location by a local density estimate would address the first issue, but not the second.\n",
    "We view the unwelcome dependence of the $\\linavg$ estimand on the border topography as a side effect of ignoring the fact that the 1-dimensional treatment function $\\tau(\\boundary)$ is embedded in a Euclidean 2-dimensional space. \n",
    "This fact is captured by the covariance structure: sentinels in the straight segment of the border will be less strongly correlated than in the sinuous segments. \n",
    "The more correlated sentinels individually carry less information about the local treatment effect. \n",
    "This suggests that instead of averaging the treatment effect evenly along the border, we wish to average evenly the information contained therein. \n",
    "This motivates the use of the inverse-variance weighted mean $\\invvar$, which efficiently extracts the information from the posterior to produce the weighted avereage with minimum variance.\n",
    "\\begin{equation}\\begin{split}\n",
    "    \\invvar \\mid Y_T, Y_C, \\sigmaf, \\sigman, \\ell &\\sim \\normal\\del{\\mu_{\\invvar \\mid Y}, \\Sigma_{\\invvar \\mid Y}} \\\\\n",
    "    \\mu_{\\invvar \\mid Y} &\\approx \\del{\\ones\\trans \\Sigma_{\\sentinels \\mid Y}^{-1} \\mu_{\\sentinels \\mid Y}} \\big/ \\del{\\ones\\trans \\Sigma_{\\sentinels \\mid Y}^{-1} \\ones}  \\\\\n",
    "    \\Sigma_{\\invvar \\mid Y} &\\approx 1 \\big/ \\del{\\ones\\trans \\Sigma_{\\sentinels \\mid Y}^{-1} \\ones}\n",
    "\\end{split}\\label{eq:invvar}\\end{equation}\n",
    "This estimator will automatically give more weight to sentinels in dense areas (as the variance will be lower there), and to sentinels in straight sections of the border. While the estimand is less clear, the approach is in keeping with the philosophy of regression discontinuity designs. We let information be our guide when averaging over our boundary, just like it guided the analysis of regression discontinuity designs to only focus on the treatment effect at the boundary. The estimand isn't chosen by the scientist, but it is dictated by the limitations of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2GP: Testing for non-zero effect\n",
    "\n",
    "Following the 2GP procedure, we might naturally wonder whether we can claim to have detected a significant treatment effect anywhere along the boundary. \n",
    "In the hypothesis testing framework, we have two possible choices of null hypotheses. \n",
    "The **sharp null** specifies that the treatment effect is zero everywhere along the boundary:\n",
    "$\\tau(\\boundary)=0$, \n",
    "while the **weak null** only requires the average treatment effect to be zero.\n",
    "\n",
    "## Using the inverse-variance weighted mean treatment effect posterior to test the weak null hypothesis\n",
    "\n",
    "As we saw in the previous section, the “average” treatment effect can be defined in multiple ways.\n",
    "If we choose the inverse-variance weighted mean, then $\\invvar$ has posterior given by \\eqref{eq:invvar}.\n",
    "While the posterior is a Bayesian object, we can use it heuristically to derive a pseudo-$p$-value\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    Z_0 &\\sim \\normal\\del{0, \\Sigma_{\\invvar \\mid Y}}  \\\\\n",
    "    p^{\\mathrm{INV}} &= \\Pr\\del{ \n",
    "        \\abs{Z_0} > \n",
    "        \\abs{\n",
    "            \\mu_{\n",
    "                \\invvar \\mid Y\n",
    "            }\n",
    "        } \n",
    "    } \\\\\n",
    "    &= 2\\Phi\\del{-\n",
    "        \\frac{\n",
    "            \\abs{\n",
    "                \\mu_{\n",
    "                    \\invvar \\mid Y\n",
    "                }\n",
    "            }\n",
    "        }{\n",
    "            \\sqrt{\n",
    "                \\Sigma_{\\invvar \\mid Y}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "While we didn't derive this pseudo-$p$-value through a rigorous procedure, our simulations show that it actually has good frequentist properties.\n",
    "\n",
    "## Likelihood-based sharp null test\n",
    "\n",
    "We can also target the sharp null hypothesis.\n",
    "We first create a null model $\\modnull$, \n",
    "specified as a single Gaussian process spanning the control and treatment regions, \n",
    "with the same kernel and hyperparameters obtained in the 2GP procedure. \n",
    "$\\modnull$ is smooth and continuous at the boundary,\n",
    "and therefore accords with the sharp null hypothesis.\n",
    "Intuitively, if there is a treatment effect, \n",
    "the likelihood of the observations should be lower under $\\modnull$ than under $\\modalt$, \n",
    "the 2$\\gp$ model as specified in equation \\eqref{eq:spec2gp}.\n",
    "We therefore choose the difference in log-likelihoods as our test statistic\n",
    "\\begin{equation}\n",
    "    t = \\log \\Pr\\del{Y_T, Y_C \\mid \\modalt} - \\log \\Pr\\del{Y_T, Y_C \\mid \\modnull}\n",
    "\\end{equation}\n",
    "and wish to reject the sharp null hypothesis when its observed value $t_{obs}$ is high.\n",
    "\n",
    "A parametric bootstrap approach is used to quantify what “high” means. We draw $Y_T^*,Y_C^*$ from $\\modnull$, \n",
    "using the same spatial locations as in the original data, \n",
    "and then fit the two competing models to the simulated data in order to obtain the bootstrapped test statistic\n",
    "\\begin{equation}\n",
    "    t^* = \\log \\Pr\\del{Y_T^*, Y_C^* \\mid \\modalt} - \\log \\Pr\\del{Y_T^*, Y_C^* \\mid \\modnull}\n",
    "\\end{equation}\n",
    "Repeating this procedure, we obtain a distribution of $t$ under $\\modnull$, \n",
    "which we can then compare to the observed $t$.\n",
    "More precisely, we can interpret the proportion of $t^*$ drawn above $t_obs$ as a $p$-value.\n",
    "\\begin{equation}\n",
    "    p^{\\mathrm{lik}} = \\Pr\\del{t^* > t_{obs} \\mid \\modnull}\n",
    "\\end{equation}\n",
    "Computationally, because the hyperparameters and locations of the units are held constant during the bootstrap, we can reuse the Cholesky decomposition of the covariance matrix, allowing the test to be performed in seconds even with hundreds of units and thousands of bootstrap samples.\n",
    "\n",
    "## $\\chi^2$ test for the sharp null\n",
    "\n",
    "The likelihood-based sharp null above is valid and easy to understand.\n",
    "But it may seem odd that the test aims to detect a non-zero treatment effect at the boundary, without any explicit reference to the boundary $\\boundary$. The test statistic and $p$-values can be computed without access to the sentinel positions, using only the treatment and control indicators.\n",
    "\n",
    "To address this oddity, we can derive a test statistic directly from the posterior treatment effect along the boundary, \n",
    "approximated in equation \\eqref{eq:postvar2gp} by its mean vector $\\muvec_{\\sentinels \\mid Y}$ \n",
    "and covariance matrix $\\Sigma_{\\sentinels \\mid Y}$ at the sentinel positions $\\sentinels$.\n",
    "We will use $\\muvec$ and $\\Sigma$ as shorthand throughout this section.\n",
    "If a $k$-vector $\\yvec$ has multivariate distribution $\\normal{0, \\Sigma}$, then $\\yvec\\trans \\Sigma^{-1} \\yvec$ has distribution $\\chi^2_k$.\n",
    "This suggests that we could use $S=\\muvec\\trans \\Sigma^{-1} \\muvec$ as a test statistic, \n",
    "and obtain a $p$-value by comparing it to a $\\chi^2_k$ distribution, where $k$ is the number of sentinels. \n",
    "However, we face two problems.\n",
    "Firstly, this test derived heuristically from a Bayesian posterior is invalid from a frequentist perspective.\n",
    "Secondly, while $\\Sigma$ is mathematically full-rank, it is typically numerically rank-deficient.\n",
    "Therefore, $k$ overestimates the true degrees of freedom of $\\Sigma$, which invalidates the test.\n",
    "\n",
    "Benavoli and Mangili (2015), developing a test for function equality, address the second problem by trimming the $\\Sigma$ eigenvalues $\\lambda_i$ lower than $\\epsilon \\sum_{j=1}^k \\lambda_j$, with $\\epsilon$ a pre-specified small number (they use 0.01).\n",
    "They address the first problem by showing that the resulting $p$-value is conservative in certain simulation settings. \n",
    "However, in our work, we found the resulting $p$-value to be sensitive to the arbitrarily chosen  $\\epsilon$ tolerance parameter, which makes it difficult to believe its validity.\n",
    "\n",
    "We therefore again take the parametric bootstrap approach, using $S$ as a test statistic rather than the likelihood ratio, and relying on julia's matrix division polyalgorithm to ensure numerical stability. We checked that adding a small constant to the diagonal of $\\Sigma$ does not greatly affect the computed $S$. Furthermore, the parametric bootstrap ensures the frequentist validity of the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power in simulated example\n",
    "\n",
    "![Position of units in an imaginary experiment in Louisiana and Mississippi](figures/mississippi_counties.png)\n",
    "$\\label{fig:mississippi_counties}$\n",
    "\n",
    "The three tests we developed leverage different aspects of the problem, and target two different null hypotheses. One may wonder how their power compares in the presence of a treatment effect. Considering once more the boundary between Louisiana and Mississippi, we imagine an experiment where the unit of analysis is the county, located at its centroid, as shown in Figure \\ref{fig:mississippi_counties}(a). \n",
    "We will simulate outcomes from a single Gaussian Process covering both states. For simplicity, we fix the hyperparameters to arbitrary values: $\\sigman=\\sigmaf=1.0$ and $\\ell=50\\,\\mathrm{km}$. We then add a constant treatment effect $\\tau$ to all the outcomes in Louisiana. The results for $\\tau=0$ (null hypothesis) and $\\tau=0.5$ are shown in Figure \\ref{fig:power}.\n",
    "\n",
    "![Power of hypothesis tests](figures/power_HT.png)\n",
    "$\\label{fig:power}$\n",
    "\n",
    "We see that under the null, the $p$-values of the $\\chi^2$ and likelihood ratio tests are uniformly distributed. This is enforced by the parametric bootstrap, which draws test statistics from the same null distribution to calibrate the tests. However, the $p$-values for the inverse-variance test are biased down, so for example if we set $\\alpha=0.05$, we will falsely reject the null $7.5\\%$ instead of $5\\%$ of the time. While unfortunate, this is unsurprising, since the inverse-variance test was derived heuristically rather than from a rigorous frequentist procedure. It can be calibrated using the same parametric bootstrap approach that was used for the likelihood and $\\chi^2$ tests. The calibration can also be achieved analytically, since $\\mu_{\\invvar \\mid Y}$ is normally distributed under the null hypothesis.\n",
    "\n",
    "Even after the calibration, the hypothesis test based on the inverse-variance mean has the highest power to detect the constant treatment effect. This can lead to a paradox: we may reject the weak null hypothesis, but fail to reject the sharp null hypothesis (using the $\\chi^2$ or likelihood test), even though rejection of the weak null should logically imply rejection of the sharp null. This paradox isn't specific to this setting, and is discussed in depth in the context of randomization-based inference by Ding (2014). Therefore, in scientific contexts where the main interest is an overall (average) increase or decrease in outcomes, we recommend using the inverse-variance test to maximize power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placebo tests\n",
    "\n",
    "Gaussian Process models are almost always misspecified. \n",
    "We do not believe that the Gaussian process with stationary squared exponential kernel is the true data-generating process, although we hope that the model is sufficiently flexible to represent reality well.\n",
    "Under misspecification, we should be skeptical of results that rely on the truth of the model specification.\n",
    "We therefore encourage practicioners to probe the validity of the above hypothesis tests by running a “placebo” test.\n",
    "A placebo test repeatedly applies the hypothesis test on data that are known to have zero treatment effect (a “placebo”),\n",
    "in order to verify that the returned p-values are uniformly distributed.\n",
    "In our spatial setting, we will use the treatment and control regions separately as placebo groups.\n",
    "Within each placebo group, we repeatedly draw an arbitrary geographical boundary, creating new treatment and control groups.\n",
    "Because the boundary was chosen arbitrarily by us, we should not expect there to be a discontinuous jump in outcomes at this boundary.\n",
    "We then apply the bootstrapped likelihood test procedure described above to this arbitrarily divided data, store the results, and hope to obtain a roughly uniform distribution of p-values.\n",
    "In our implementation, we drew lines that split the placebo units in half at a sequence of angles $1\\degree,2\\degree,3\\degree,\\ldots,180\\degree$.\n",
    "The resulting p-values will obviously be highly correlated, so we should only expect a very roughly uniform distribution (because of the small effective sample size), but at the very least, this procedure allows us to visually verify that the p-values are not blatantly biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial advantage\n",
    "\n",
    "Classical regression discontinuity designs often suffer from low power, requiring many units near the boundary for inference to be possible. \n",
    "In the spatial RDD setting, we might worry that the situation is worse, as geographical datasets with many units packed along the boundary are uncommon. \n",
    "In geographical settings, each unit (e.g. household or counties) normally takes up space, so there is a limit to how densely packed units can be near the boundary. \n",
    "And boundaries often include sparsely populated segments, e.g. running through parks, industrial areas, or farmland. \n",
    "The intuition that spatial RDDs will therefore suffer from low power is correct, inasmuch as at any given point along the boundary, the posterior variance of $\\tau(\\boundary)$ will typically be high. \n",
    "But once we pool the information into an average treatment effect, or perform a sharp test, spatial RDDs can be more powerful than classical RDDs, with the same number of units at the same distance from the boundary.\n",
    "\n",
    "We illustrate this statement once more with the Louisiana-Mississippi example.\n",
    "The variance of the inverse-variance weighted treatment effect $\\invvar$ is thence only a function of the positions of the units, available analytically by plugging the posterior variance \\eqref{eq:postvar2gp} into the inverse-variance estimator \\eqref{eq:invvar}. \n",
    "Following this procedure, we obtain a posterior standard deviation of the average treatment effect of 0.31. \n",
    "We then create a one-dimensional regression discontinuity design for the same setting, by using each unit's distance from the boundary as the covariate $x$, the distribution of which is shown in Figure \\ref{fig:mississippi_counties}(b). \n",
    "Following the exact same 2GP procedure with the same hyperparameters as in the spatial setting, and with a discontinuity at $x=0$, we again compute the posterior standard deviation of the treatment effect at the boundary (now a single number rather than a continuous function) , this time obtaining 0.58. \n",
    "This higher figure indicates that, perhaps counter-intuitively, the spatial experiment actually has more power than its one-dimensional analog.\n",
    "\n",
    "To gain intuition about the higher power of the spatial RDD, we turn to the interpretation of regression discontinuity designs as natural experiments [need reference]. \n",
    "Near the discontinuity, we can reasonably claim that the side of the discontinuity that each unit fell into was largely dictated by random noise in the covariate. \n",
    "This in turn allows us to claim that a natural randomized experiment took place near the boundary, with treatment and control units coming from the same population. \n",
    "We can extend this interpretation to the spatial setting, by conceiving of multiple correlated experiments taking place all along the boundary. \n",
    "The average treatment effect estimator then pools the information supplied by all of these experiments.\n",
    "The question then becomes: do we get more powerful inference by grouping all the units into a single experiment, or by spreading them along a multitude of weaker experiments?\n",
    "There are two sources of uncertainty in our model: the observation noise $\\epsilon_i$, and the underlying processes $g_T$ and $g_C$. Adding more units to a single experiment allows us to cancel out more of the observation noise, but if the new units aren't added closer to the discontinuity, uncertainty always remains in $g_T$ and $g_C$. In the spatial setting, however, we observe multiple realizations of the Gaussian process, and therefore do not suffer from the same diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: NYC school districts\n",
    "\n",
    "We illustrate the analysis of geographical regression discontinuity designs using house sales data from New York City. \n",
    "The city publishes information pertaining to property sales within the city in the last twelve months on a rolling basis. \n",
    "This includes the sale price, building class, and the address of the property.\n",
    "Public schools in the city are all part of the City School District of the City of New York, the district is itself divided into 32 sub-districts.\n",
    "Within districts, schools also have attendance zones, and children living within a zone are guaranteed attendance in their zone school unless the school is full [is this true? [insideschools.com gives a more complete picture](http://insideschools.org/elementary/how-to-apply)].\n",
    "It is commonly held [could cite [this article at cityrealty.com](https://www.cityrealty.com/nyc/market-insight/features/trending-in-ny/buying-renting-school-zone-district-what-you-need-know/3661)] that school districts therefore have an impact on real estate price, as parents are willing to pay more to live in districts with better schools. \n",
    "We therefore ask: can we measure a discontinuous jump in house prices across school district boundaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "In order to model the property sale prices with a stationary Gaussian process, we need to obtain their location on a Euclidean grid. We geocode the address of each sale by merging the sales with NYC's Pluto database, which contains X and Y coordinates for each house, identified by its borough, zip code, block and lot. These coordinates are given in the `EPSG:2263` projection in units of feet. We use this projection throughout this example. For addresses that do not find a match in Pluto, we use google's geocoding API to obtain a latitude and longitude, which we then project to `EPSG:2263`.\n",
    "\n",
    "We then filter the sales data as follows, by removing\n",
    "1. sales of properties without a reported sale price\n",
    "1. sales of properties outside of the residential building class categories (“one family dwellings”, “two family dwellings”, “three family dwellings”, “tax class 1 condos”, “coops - walkup apartments”, “coops - elevator apartments”, “condos - walkup apartments”, “condos - elevator apartments”, “condos - 2-10 unit residential”, “condo coops\"),\n",
    "2. any sale with missing data in the sale price, square footage, property covariates, geographical coordinates (due to failed geocoding),\n",
    "3. sales outside of any NYC school district,\n",
    "4. properties smaller than 100 sq ft, and\n",
    "5. outliers in the price per square foot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "![sales map](NYC/NYC_plots/NYC_sales.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for property prices\n",
    "\n",
    "The outcome of interest is price per square foot. As is often done in the real estate literature, we take its logarithm to reduce the skew of the outcome. The complete model is then a Gaussian Process over the geographical covariates $\\svec$ super-imposed with a linear regression on the property covariates (building and tax class). Within a school district we could write the model as [suggestions for clearer notation welcome]:\n",
    "\n",
    "\\begin{align}\n",
    "    Y_i &= \\log\\del{ \\frac{\\saleprice_i}{\\sqft_i}} = \\beta_0 + \\beta_{1,\\tax\\sbr{i}} + \\beta_{2,\\building\\sbr{i}} + f(\\svec_i) + \\epsilon_i \\\\\n",
    "    \\epsilon_i &\\sim \\normal\\del{0, \\sigma_y^2} \\\\\n",
    "    \\beta_{1j},\\beta_{2j} &\\sim \\normal\\del{0, \\sigma_\\beta^2} \\\\\n",
    "    f(\\svec_i) &\\sim \\gp\\del{0, k(\\svec, \\svec')} \\\\\n",
    "    k(\\svec, \\svec') &= \\sigmaf^2 \\exp\\cbr{ - \\frac{(\\svec-\\svec')\\trans(\\svec-\\svec')}{2\\ell^2}}\n",
    "\\end{align}\n",
    "\n",
    "A visual inspection of the house sales map above led me to focus on the boundary between districts 19 and 27. I found a map online of average maths performance in each school district, which shows that districts 19 and 27 are quite different. It's important to note that the boundary between the two districts is also part of the boundary between Brooklyn and Queens, so we won't be able to attribute a causal effect solely to the difference in school districts. A histogram of $Y$ in both districts also shows that marginally the house prices are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter optimization\n",
    "\n",
    "## cliff face\n",
    "\n",
    "### surface plots\n",
    "\n",
    "## average effect\n",
    "\n",
    "## significance tests\n",
    "\n",
    "### placebo tests\n",
    "\n",
    "## pairwise treatment effect (all districts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "974px",
   "left": "0px",
   "right": "746px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
