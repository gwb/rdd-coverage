\section{Introduction}\label{sec:geordd_introduction}

\label{sec:framework}
It is commonly believed that the quality of schools has a significant effect on the price of nearby housing.
Economists have long been interested in measuring and using this effect to estimate the implicit price of school quality.
\citet{black2011housing} review the existing literature on this topic, and classify the various identification strategies and methodologies that have been proposed.
Typically, the theoretical underpinning of this literature is the hedonic valuation model \citep{rosen1974hedonic,sheppard1999hedonic}, whereby the price of a house is decomposed into (usually log-linear) contributions from characteristics of the house, of the neighborhood, and of the schools that local residents can access.
The estimation of the causal effect of a unit of school quality on the price of a house is made difficult by unobserved neighborhood characteristics which correlate with both school quality and house prices.
Consequently, ordinary least squares (OLS) estimates are confounded by these unobserved factors \citep{black2011housing}.

Starting with \citet{black1999better}, economists have recognized the opportunity offered by public school systems that rely on attendance districts to assign children to schools.
If houses on opposite sides of the boundaries between districts have access to the same neighborhood amenities (public transport, parks, etc.) except for schools, then the price difference between otherwise similar houses can plausibly be attributed to difference in the quality of the schools that their residents can access.
This strategy to identify and estimate the causal effect of school quality can be understood as a regression discontinuity design (RDD).
RDDs are natural experiments characterized by the treatment assignment being fully determined by some covariates, which are termed ``forcing'' variables.
A typical RDD scenario arises when a treatment is given to all units with a forcing variable that falls below (or above) an arbitrary threshold value, and is withheld from units on the other side of the threshold.
If, as is often the case, the forcing variable is also predictive of the outcome of interest, then treatment assignment and outcomes are confounded, but by focusing on units near the threshold, a causal treatment effect can nonetheless be estimated.
The theory and methods for RDDs date from the 1960s, starting with \citet{thistlethwaite1960regression}.
\citet{cook2008waiting} trace the history of how interest in RDDs subsequently waned, until the late 1990s when the design saw renewed attention, theoretical progress, and applications in the social sciences.

For house sales, the two forcing variables are their geographical coordinates, and the threshold is the border between attendance districts.
Until relatively recently, most theory and applications of RDDs were for univariate cases,
but beginning with \citet{papay2011extending}, methods have been developed to analyze multivariate RDDs.
\citet{imbens2011regression} extend the local linear regression methods \citep[see][]{imbensrdd} that are popular for analyzing univariate RDDs (1D~RDDs) to settings with multiple forcing variables.
% Most germane to this paper are the contributions of \citet{black1999better}, who is the first to deploy the regression discontinuity design (RDD) in this context, and \citet{gibbons2013valuing} and \citet{fack2010better}, who anticipate \citet{keele2015enhancing} in proposing matching methodologies to address the issue of spatial confounding.
When these forcing variables are spatial, i.e. treatment and control units are separated by a geographical border, this becomes a Geographical regression discontinuity design (GeoRDD).
% meaning that units within a certain region are assigned to treatment, while units in a neighboring region are assigned to control.
A convenient approach to GeoRDDs sometimes seen in applied work \citep[e.g.][]{macdonald2015effect,chen2013evidence,magruder2012high,holmes1998effect} is to 
reduce it to a 1D~RDD by using the signed minimum distance to the boundary (positive for treatment and negative for control) as the forcing variable, a method we refer to as ``projected 1D~RDD.''
But this can fail to capture the spatial variation in outcomes, resulting in a confounded estimator:
see \autorefexternal{sec:confounding}\ofsupp{} and
Section~4.2 of \citet{keele_titiunik_2015}.
Firmer theoretical foundations for GeoRDDs are built by \citet{keele_titiunik_2015},
who extend the identification assumptions that were formalized by \citet{hahn2001identification} for 1D~RDDs, and by \citet{imbens2011regression} for multivariate RDDs.
To estimate the treatment effect, \citet{keele_titiunik_2015} and \citet{keeleoverview} apply the projected 1D~RDD method locally around points on the border, thus alleviating the problem of spatial confounding.
% For example, in \citet{macdonald2015effect}, a private police force patrols a neighborhood, but stays out of surrounding areas, and a causal effect on crime rates is sought.
% In \citet{chen2013evidence}, a policy applies south of the Huai River in China but not in the north, and pollution levels and life expectancies are measured to infer environmental and health impacts of the policy.
For valuing school quality, \citet{gibbons2013valuing} and \citet{fack2010better} propose matching methodologies to address the issue of spatial confounding.
By matching similar units on opposite sides of the border that are near each other geographically, the difference in their outcomes can plausibly be attributed to the presence of the border.
Similarly, \citet{keele2015enhancing} use the matching methods of \citet{zubizarreta2012using} in a GeoRDD to estimate the effect of ballot initiatives on voter turnout in Milwaukee, Wisconsin.

Spatial statistics, the branch of statistics dedicated to inference for geographical units with spatially correlated outcomes, has been mostly absent from this literature.
In this paper, we propose a framework for analyzing GeoRDDs that is a spatial analogue of 1D~RDD methods.
Broadly, 1D~RDD methodologies \citep{imbensrdd} are composed of three steps:
\begin{flatlist}
    \item
        fit a smooth \emph{function} to the outcomes against the forcing variable on each side of the threshold,
    \item
        extrapolate the functions to the \emph{threshold point}, and
    \item
        take the difference between the two extrapolations to estimate the treatment effect at the threshold point.
\end{flatlist}
Reusing the same methodological skeleton and applying it to geographical RDDs, our framework proceeds analogously:
\begin{flatlist}
    \item
        fit a smooth \emph{surface} to the outcomes against the geographical covariates in each region,
    \item
        extrapolate the surfaces to the \emph{border curve}, and
    \item
        take the \emph{pointwise} difference between the two extrapolations to estimate the treatment effect along the border.
\end{flatlist}
The usefulness of spatial models is then evident;
we propose to use kriging, also known as Gaussian process regression (GPR), to fit and extrapolate the outcomes, 
but other spatial methods could also be suitable.
For 1D~RDDs, \citet{Branson:2017qy} proposed a GPR methodology that exhibits promising coverage and MSE properties compared to local linear regression.
We believe this approach to be particularly suitable to GeoRDDs, as GPR is a well-established tool in spatial statistics for fitting smoothly varying spatial processes.
See \citet{banerjee2014hierarchical} for a textbook introduction to kriging for spatial data, and \citet{rasmussen2006gaussian} for a machine learning perspective.

Our implementation of the methods proposed in this paper uses the \texttt{GaussianProcesses.jl} package \citep{fairbrother2018gaussianprocesses} for the julia programming language \citep{bezanson2017julia} and is made available on the first author's GitHub account.

\autoref{sec:geordd_estimation} explains our GeoRDD methodology.
In \autoref{sec:gpmodel}, we use GPR to estimate the treatment effect along the border by extending the model of \citet{Branson:2017qy} to geographical settings.
A peculiarity of GeoRDDs is that the estimand is a function defined everywhere along the border, which is a one-dimensional manifold embedded in two-dimensional space.
Furthermore, geographical borders, whether they be political or natural, are rarely simple straight lines.
The topology of borders complicates the definition and interpretation of estimands for the local average treatment effect (LATE), which we address in \autoref{sec:ate}.
We obtain Bayesian estimators for multiple possible LATE estimands and discuss their properties.
In \autoref{sec:hypothesis_testing} we turn to hypothesis testing, and propose a method to test against the null hypothesis of no treatment effect along the border.

In \autoref{sec:NYC_example}, our methodology is applied to the problem of valuing school quality, 
using publicly available data of property sales in NYC to determine whether school districts affect property prices.
Focusing on a single border between two school districts, we estimate the treatment effect everywhere along the border, obtain estimates of the LATE, and perform and validate a hypothesis test.
We find a statistically significant difference in price across the border with a \(p\)-value of 0.002, and estimate that the same house located near the border will on average fetch an almost 20\% higher price in district 27 than in district 19.
However, this effect can not be attributed solely to the quality or reputation of the schools, as this border also separates the boroughs of Brooklyn and Queens, thus confounding the causal effect of the districts.

% In our application, we seek to estimate the effect of school districts on house prices in New York City.

% The main requirement for identification of the treatment effect is continuity of the conditional regression functions near the border.
% Notably, this is violated if units can sort around the border, crossing the border to seek or avoid the treatment, a particular concern in GeoRDDs.
% \citet{keele_titiunik_2015} discuss further pitfalls of GeoRDDs, such as the issue of compound treatments---when a geographical border determines the assignment of the treatment of interest, but also of other differences.

\section{GeoRDD estimation with Gaussian processes}
\label{sec:geordd_estimation}

We largely adopt the setup and notation for GeoRDDs laid out in \citet{keele_titiunik_2015}.
The outcomes \(Y_i\) of \(n\) units with spatial coordinates \(\svec_i\) are observed within an area \(\area\) of 2-dimensional coordinate space.
The units are separated into \(n_\treat\) treatment units in area \(\treatarea \subset \area\)
and \(n_\ctrol\) units in the control area \(\ctrolarea\).
The defining characteristic of GeoRDDs is that the two areas are adjacent but non-overlapping, intersecting only at the border \(\border\) between them.
Throughout this paper, points on the border are denoted by \(\sentinel\).
Under the potential outcomes framework for causal inference, each unit \(i\) has potential outcomes \(Y_{i\treat}\) and \(Y_{i\ctrol}\) under treatment and control respectively.
Let \(\treatind_i\) denote the treatment indicator, which is equal to one if unit \(i\) is in the treatment area, and zero if it is in the control area.
Unlike traditional randomized experiments, treatment assignment is a deterministic function of a unit's geographical coordinates \(\svec_i\): \(\treatind_i = \Ind\cbr{\svec_i \in \treatarea}\).
The observed outcome for unit \(i\) is \(Y_i = \treatind_i Y_{i\treat} + (1 - \treatind_i) Y_{i\ctrol}\).
We denote the vector of observed outcomes of the treatment units and control units respectively by \(\yt\) and \(\yc\), and \(\Yvec\) the vector formed by concatenating \(\yt\) and \(\yc\).

For 1D RDDs, because the treatment and control regions do not overlap, the treatment effect is typically only inferred at the threshold \(X=b\).
As was already recognized by \citet{thistlethwaite1960regression}, this choice requires the least extrapolation of the fitted regression functions, which makes the estimated treatment more credible.
The estimand at the threshold can be obtained as the difference of the two limits of the expectation of the conditional regression functions
\begin{equation}
    \tau = \E\sbr{Y_{i\treat} \mid X_i=b} - \E\sbr{Y_{i\ctrol} \mid X_i=b} = \lim_{x \downarrow b} \E\sbr{Y \mid X=x} - \lim_{x \uparrow b} \E\sbr{Y \mid X=x}\,,
    \label{eq:rdd_univ_estimand}
\end{equation}
where the second equality requires the assumption that the conditional regression functions \(\E\sbr{Y_{i\treat} \mid X_i=x}\) and \(\E\sbr{Y_{i\ctrol} \mid X_i=x}\) are continuous in \(x\) (see Assumption 2.1 in \citet{imbensrdd} and the discussion that follows).
Analogously, we focus on the treatment effect at the border \(\border\) between the treatment and control regions:
\begin{equation}
\tau\colon \border \rightarrow \mathbb{R}
\quad\text{defined as}\quad
\tau\del{\sentinel} = \E\sbr{Y_{i\treat} - Y_{i\ctrol} \mid \svec_i = \sentinel}
\,.
\label{eq:functional_estimand}
\end{equation}
This is the functional estimand defined in \citet{imbens2011regression} and \citet{keele_titiunik_2015}.
For any \(\sentinel \in \border\), \(\tau(\sentinel)\) can be obtained as the difference of the two limits of the expected outcomes, approaching \(\sentinel\) from the treatment or the control side of the border, given the assumption that the conditional regression functions \(\E\sbr{Y_{i\treat} \mid \svec_i=\svec}\) and \(\E\sbr{Y_{i\ctrol} \mid \svec_i=\svec}\) are continuous in \(\svec\) within \(\area\).
This result is formalized under Assumption 2.2.2 by \citet{imbens2011regression} and Assumption 1 in \citet{keele_titiunik_2015}.

For computational reasons, we often represent the border as a set \(\sentinels=\sentinelset\), \(\sentinel_\isent \in \border\) of \(\numsent\) ``sentinel points'' along the border.
We denote by \(\tau(\sentinels)\) the \(\numsent\)-vector with \(r^\mathrm{th}\) entry \(\tau(\sentinel_r)\) of the treatment effect evaluated at \(\sentinel_r\).


\subsection{Model Specification}
\label{sec:gpmodel}
Our GeoRDD framework allows any method to be used to fit the outcomes on either side of the border.
In this paper we use GPR for this purpose.
GPR, known as kriging in the spatial statistics literature, is a Bayesian non-parametric method for fitting smooth functions. 
Recently, \citet{Branson:2017qy} showed GPR to be a promising approach for the analysis 1D RDDs.
Further inspired by the popularity of GPR in spatial statistics, we extend the model of \citet{Branson:2017qy} to geographical RDDs.

On each side of the border, we model the observed outcomes \(Y_i\) at location \(\svec_i\) as the sum of an intercept \(m\), a spatial Gaussian process \(f(\svec)\), and \iid{} normal noise \(\epsilon\).
The Gaussian process has zero mean, and its covariance function is a modeling choice.
There is a rich literature of possible covariance functions, known as ``kernels'' in machine learning; see
\citet{banerjee2014hierarchical} and \citet{rasmussen2006gaussian} for examples.
In this paper we use the Mat\'ern \(\nu=1/2\) covariance (also known as the exponential kernel) for its ease of understanding and its prevalence in applied spatial statistics.
This yields the outcomes model:
\begin{equation}
    \begin{split}
        & Y_{i\treat} = \underbrace{m_\treat{} + f_\treat{}(\svec_i)}_{g_\treat{}(\svec_i)} + \epsilon_i \quad\text{and}\quad
          Y_{i\ctrol} = \underbrace{m_\ctrol{} + f_\ctrol{}(\svec_i)}_{g_\ctrol{}(\svec_i)} + \epsilon_i 
          \,,\quad\text{with } \epsilon_i \overset{\indep}{\sim} \normal(0, \sigman^2) \,; \\
        & f_\treat{}, f_\ctrol{} \overset{\indep}{\sim} \GP\del{0, k(\svec, \svec')} \quad\text{with}\quad
        k(\svec,\svec') = \sigmaf^2 \exp\del{-\norm{\svec-\svec'}_2 \big/ \ell} \,.
    \end{split}
    \label{eq:spec2gp}
\end{equation}
The treatment effect at a location \(\sentinel\) on the border is derived as the difference between the two noise-free surfaces \(g_\treat{}\) and \(g_\ctrol{}\):
\begin{equation}
    \tau(\sentinel) = \sbr{m_\treat{} + f_\treat{}(\sentinel)} - \sbr{m_\ctrol{} + f_\ctrol{}(\sentinel)}\,.
\end{equation}
This can be visualized as the height of a cliff along the border \(\border\) separating the two smooth plains of the treatment and control regions.

In this specification, the hyperparameters \(\ell\), \(\sigmaf\), and \(\sigman\) are the same in the treatment and control regions, so we assume that the spatial smoothness of the responses is not affected by the treatment.
We expect that this assumption will be reasonable in many applications, but it can be easily relaxed, as discussed in \citet{Branson:2017qy}.

\subsection{Inference of the Treatment Effect}
\label{sec:inference}
If \(m_\treat\) and \(m_\ctrol\) are given normal priors with variance \(\sigmamu^2\), then the model specification \autoref{eq:spec2gp} can be used to obtain covariances between the observations, the Gaussian processes, and the mean parameters.
Given hyperparameters \(\hyperparam = \del{\ell,\sigmaf, \sigman, \sigmamu}\), any vector with entries consisting of observations, points on the potential outcomes surface \(f_{\treat}\) and \(f_{\ctrol}\), and the mean parameters \(m_{\ctrol}, m_{\treat}\) is jointly multivariate normal. Therefore the distribution of any such vector conditioned on another is also multivariate normal, with mean and covariances analytically tractable, and easily computed.

In accordance with the framework laid out in \autoref{sec:framework}, we proceed by extrapolating both Gaussian processes to the border,
and then taking the difference of the predictions to obtain the posterior treatment effect along the border.
Computationally, we need to represent this border as a set \(\sentinels=\sentinelset\) of \(\numsent\) ``sentinel'' units dotted along \(\border\).
The extrapolation step then follows mechanically through multivariate normal theory.
On the treatment side, for example:
\begin{equation}\begin{split}
    & g_\treat{}(\sentinels) \mid \yt{}, \hyperparam \sim \normal\del{\muvec_{\sentinels \mid T}, \Sigma_{\sentinels \mid T}} \,\text{, with} \\
    & \muvec_{\sentinels \mid T} =
    \KBT
    \STT^{-1} 
    \yt{} 
    \quad\text{and}\quad
    \Sigma_{\sentinels \mid T} =
    \KBB - \KBT \STT^{-1} \KBT\trans \,.
\end{split}
\label{eq:postvar2gp_t_or_c}
\end{equation}
with all the covariance matrices derived from the model specification (see \autoref{sec:covariances}).
Analogously, predictions for \(g_\ctrol{}(\sentinels)\) are obtained using the data in the control region,
and their posterior mean and covariance denoted respectively by \(\muvec_{\sentinels \mid C}\) and \(\Sigma_{\sentinels \mid C}\).
Since the two surfaces are modeled as independent, the treatment effect \(\tau(\sentinels)=g_\treat{}(\sentinels)-g_\ctrol{}(\sentinels)\) has posterior
\begin{equation}
    \begin{split}
        & \tau(\sentinels) \mid \Yvec, \hyperparam \sim \normal\del{\muvec_{\sentinels \mid Y}, \Sigma_{\sentinels \mid Y}} \,\text{, with}\\
        & \muvec_{\sentinels \mid Y} = \muvec_{\sentinels \mid \treat} - \muvec_{\sentinels \mid \ctrol} \quad\text{and}\quad
        \Sigma_{\sentinels \mid Y} = \Sigma_{\sentinels \mid \treat} + \Sigma_{\sentinels \mid \ctrol} \,.
    \end{split}
    \label{eq:postvar2gp}
\end{equation}
The posterior mean and covariance of \(\tau(\sentinels)\) are the primary output of our GeoRDD analysis, and we refer to \autoref{eq:postvar2gp} as the ``cliff height'' estimator.

This leaves the choice of the hyperparameters: \(\hyperparam=\ell\), \(\sigmaf\), \(\sigman\), and \(\sigmamu\).
For \(\sigmamu\), we arbitrarily pick a large number, so that the prior on the mean parameters is weak.
The rest are optimized by maximizing the marginal likelihood of the observations \(\prob\del{\Yvec \mid \ell, \sigmaf, \sigman}\), which is available analytically and easily computed for GPR.
This empirical Bayes approach is common in spatial and machine learning applications of Gaussian processes.
An alternative would be to also specify a prior on the hyperparameters, which would be preferable in order to fully account for the uncertainty in the model, but fully Bayesian inference of large Gaussian process models tends to be computationally expensive.


\subsection{Estimating the Local Average Treatment Effect}
\label{sec:ate}

Once we have obtained the posterior of \(\tau\) \eqref{eq:postvar2gp}, estimating the local average treatment effect (LATE) along the border will often be of interest.
We consider the class of weighted means of the functional treatment effect \(\tau\del{\sentinel}\),
with weight function \(\weightb(\sentinel)\) defined everywhere on the border \(\border\).
The weighted mean integral can be approximated as a weighted sum at the sentinels \(\sentinels\):
\begin{equation}
    \tauw = \frac{\oint_\border \left. \weightb(\sentinel) \tau(\sentinel) \dif\sentinel \right.}
    {\oint_\border \left. \weightb(\sentinel) \dif\sentinel \right.}
    \approx \frac{\sum_{\isent=1}^\numsent \weightb(\sentinel_\isent) \tau(\sentinel_\isent)}
    {\sum_{\isent=1}^\numsent \weightb(\sentinel_\isent) } \,.
\label{eq:weighted_estimand}
\end{equation}
Note that the approximation assumes that the sentinels are evenly spaced, otherwise each term in the sum needs to be re-weighted by the length of the border that the sentinel occupies.
We have shown the posterior distribution of \(\tau(\sentinels)\) to be multivariate normal, with mean \(\muvec_{\sentinels \mid Y}\) and covariance \(\Sigma_{\sentinels \mid Y}\) given in \autoref{eq:postvar2gp}.
Since \(\tauw\) is a linear combination of \(\tau(\sentinels)\), its posterior is normal, with mean \(\mu_{\tauw \mid Y}\) and variance \(\Sigma_{\tauw \mid Y}\) given by
\begin{equation}
    \mu_{\tauw \mid Y} = \frac{\weightb(\sentinels)\trans \muvec_{\sentinels \mid Y}}
    {\weightb(\sentinels)\trans  \ones_\numsent}\quad\text{and}\quad
    \Sigma_{\tauw \mid Y} = \frac{\weightb(\sentinels)\trans \Sigma_{\sentinels \mid Y} \weightb(\sentinels)}
    { \del{\weightb(\sentinels)\trans  \ones_\numsent }^2 } \,,
\label{eq:weighted_estimator}
\end{equation}
where \(\weightb(\sentinels)\) is the \(\numsent\)-vector of weights evaluated at the sentinels, and \(\ones_\numsent\) is an \(\numsent\)-vector of ones.
For each estimator obtained in \autoref{eq:weighted_estimator} as a weighted mean of \(\muvec_{\sentinels \mid Y}\), we consider the ``natural'' estimand to be the same weighted mean applied to the true \(\tau\), given by \autoref{eq:weighted_estimand}.

An alternative perspective on these estimators is given by the weights induced on the observations.
Indeed, combining equations \autoref{eq:postvar2gp_t_or_c}, \autoref{eq:postvar2gp}, and \autoref{eq:weighted_estimator}, we obtain that the posterior mean of \(\tauw\) is a linear combination
\begin{equation}
    \E\del{\tauw \mid \Yvec} = \wt\trans \yt + \wc\trans \yc
    \label{eq:unit_weights}
\end{equation}
of the observed data, with ``unit weights'' given by
\begin{equation}
        \wt = \frac{
            \STT^{-1} 
            \KBT\trans \weightb(\sentinels)
        }{
            \weightb(\sentinels)\trans \ones_{\numsent}
        }
        \quad\text{and}\quad
        \wc = \frac{
            -
            \SCC^{-1} 
            \KBC\trans \weightb(\sentinels)
        }{
            \weightb(\sentinels)\trans \ones_{\numsent}
        }
        \,,
    \label{eq:unit_weights_gp}
\end{equation}
for treatment and control units respectively.

We next motivate and consider four possible choices of \(\weightb(\sentinel)\), and explore interpretations, advantages, and drawbacks. 
In \autorefexternal{sec:additional_late}\ofsupp{}, we discuss two further choices, the projected land LATE \(\taugeo\), and the projected superpopulation LATE \(\taupop\).
We also provide a simulation study to better understand the characteristics of the different LATE choices.
A summary of their properties is provided in \autoref{table:estimator_properties}.

% \subsubsection{Uniform LATE}
The simplest choice is uniform weights \(\weightb(\sentinel)=1\), a seemingly reasonable and unopinionated decision.
The uniformly weighted LATE \(\unifavg\) is estimated by averaging the entries of the mean posterior at the sentinels.
Following \autoref{eq:weighted_estimand} and \autoref{eq:weighted_estimator}:
\begin{equation}\begin{split}
    &\unifavg = \oint_\border \tau(\sentinel) \dif\sentinel
        \ \big/ \ 
        \oint_\border \dif{\sentinel}  \,, \\
    &\unifavg \mid \Yvec, \hyperparam \sim \normal\del{\mu_{\unifavg \mid Y}, \Sigma_{\unifavg \mid Y}}\,\text{, with} \\
    &\mu_{\unifavg \mid Y} = \del{\ones_{\numsent}\trans \muvec_{\sentinels \mid Y}} / \numsent \quad\text{and}\quad
    \Sigma_{\unifavg \mid Y} = \del{\ones_{\numsent}\trans \Sigma_{\sentinels \mid Y} \ones_{\numsent}} / \numsent^2 \,.
\end{split}
\label{eq:unifavg}
\end{equation}
The uniformly weighted estimand takes on a geometric interpretation: equal-length segments of the border are given equal weight.
Unfortunately, uniform weights suffer from two issues that we now describe and address.
%in \autoref{sec:tau_rho} and \autoref{sec:invvar}.

% \subsubsection{Density Weighted LATE}
% \label{sec:tau_rho}

With uniform border weights, parts of the border adjoining dense populations are given equal weights to those in sparsely populated areas.
But if the border goes through an unpopulated area, such as a lake or a public park, then the treatment effect there has little meaning and importance.
Furthermore, \(\tau(\sentinel)\) in these empty areas will have large posterior variances, which will dominate the posterior variance of \(\unifavg\), potentially jeopardizing the successful detection of otherwise strong treatment effects.

We can address this issue by weighting the treatment effect at each sentinel location by the local population density \(\rho\),
i.e.\ choosing \(\weightb(\sentinel) = \rho(\sentinel)\).
Attractively, the estimand is interpretable as the average treatment effect for the superpopulation of units that live on the border:
\begin{equation}
    \taurho = \E\sbr{Y_{i\treat} - Y_{i\ctrol} \mid \svec_i \in \border}\,.
\end{equation}
It therefore better captures the ``typical'' treatment effect received by a unit than the uniformly weighted estimand.
This is the estimand used by \citet{keele_titiunik_2015}, who themselves follow in the footsteps of \citet{imbens2011regression}.

In practice, the local density needs to be estimated.
A simple kernel density estimator can be used,
or any spatial point process model.
Strictly speaking, the uncertainty of the local density estimate should then be propagated to the estimate of \(\taurho\), which may therefore no longer have a normally distributed or analytically tractable posterior.

Furthermore, the uniform and density-weighted estimators are both undesirably susceptible to the topology of the border.
If a section of the border has more twists and turns---for example if it follows the course of a meandering river---then that section will receive disproportionately more sentinels.
The unweighted and density-weighted mean treatment estimands are both affected by this effect,
which gives higher weight to wigglier sections of the border.
See \autorefexternal{sec:wiggly_border}\ofsupp{} for a simulation demonstrating this susceptibility to border topology.
% Consider, for illustration purposes, the border separating the two American states of Louisian and Mississippi, depicted in \autorefexternal{fig:mississippi_counties}\ofsupp{}.
% From North to South, it follows the meandering Mississippi river, then takes a sharp turn to the East and becomes a straight line, until it meets the even more sinuous Pearl river, which it follows until it reaches the Gulf of Mexico.
% Consequently, sentinels placed at equal distance intervals along this border will be more densely packed along the rivers, and sparsest along the straight segment.
% When averaging a function over the border, these sections become overrepresented.
% Troublingly, the sinuousness of the border therefore determines the estimand, even though the outcomes of interest will generally have nothing to do with river topologies.
% In population terms, the result is that units near wigglier segments receive more weight.
% Worse, the resolution of the border map used for the analysis affects the estimated LATE.


% \subsubsection{Inverse-variance Weighted LATE}
% \label{sec:invvar}

% \begin{figure}[tbp]
    % \centering
    % % \includegraphics{mississippi_counts.pdf}
    % \includegraphics[height=0.5\textheight]{mississippi_counts.pdf}
    % \caption{\label{fig:mississippi_counts} 
        % Evenly spaced sentinels along the border between Mississippi and Louisiana.
        % From north to south, the border first follows the Mississippi river, becomes a straight line from west to east, and then follows the Pearl river.
        % Placing equispaced sentinels along the border results in the overrepresentation of the more sinuous sections of the border.
    % }
% \end{figure}

This unwelcome dependence of the \(\unifavg\) and \(\taurho\) estimands on the border topology is a symptom of the geometry of the GeoRDD: the border treatment effect function \autoref{eq:functional_estimand} is defined on a 1-dimensional manifold \(\border\), which itself is embedded in a Euclidean 2-dimensional space.
The dependencies induced by this geometry are reflected in the covariance \(\Sigma_{\sentinels \mid Y}\): neighboring sentinels on a straight segment of the border will be less strongly correlated with each other than those on a sinuous segment.
The more correlated sentinels individually carry less information about the local treatment effect.
Instead of averaging the posterior treatment effect along the border based on geometry or population, we consider averaging the information contained therein.
This motivates the inverse-variance weighted mean \(\invvar\):
\begin{equation}
    \begin{split}
        & \left. \invvar \mid \Yvec, \hyperparam \right. \sim \normal\del{\mu_{\invvar \mid Y}, \Sigma_{\invvar \mid Y}} \,\text{, with} \\
        & \mu_{\invvar \mid Y} = \del{\ones_{\numsent}\trans \Sigma_{\sentinels \mid Y}^{-1} \muvec_{\sentinels \mid Y}} \big/ \del{\ones_{\numsent}\trans \Sigma_{\sentinels \mid Y}^{-1} \ones_{\numsent}} \quad\text{and}\quad
        \Sigma_{\invvar \mid Y} = 1 \big/ \del{\ones_{\numsent}\trans \Sigma_{\sentinels \mid Y}^{-1} \ones_{\numsent}} \,.
    \end{split}
    \label{eq:invvar}
\end{equation}
This estimator efficiently extracts the information from the posterior treatment effect, as it can be shown to minimize the posterior variance among weighted averages of the form \autoref{eq:weighted_estimand}.
It automatically gives more weight to sentinels in dense areas (as the variance will be lower there), and to sentinels in straight sections of the border (as the correlations between sentinels will be lower).

The estimand is still a weighted mean, with weights for the sentinels given by \(\weightb(\sentinels) = \Sigma_{\sentinels \mid Y}^{-1} \ones_{\numsent}\).
This can put negative weights on some sentinels, and this estimand does not lend itself to an intuitive interpretation.
It is not chosen on scientific grounds, but rather dictated by the observed data.
This is counter to the conventional approach in causal inference, that the estimand should be chosen based on substantive grounds, ideally before collecting any data.
While perhaps unorthodox, analogous ``estimands of convenience'' have been proposed in other settings, for example matching methods that exclude some unmatched units from the analysis \citep[discussed in][]{crump2009dealing}, or in the context of balancing treatment and control populations with little overlap in their covariate distributions \citep{li2016balancing}.
The 1D RDD could be said to provide another example, as the estimand \autoref{eq:rdd_univ_estimand} focuses on the treatment effect near the threshold not because these units are of particular substantive interest, but because the available data restricts estimation of the treatment effect elsewhere.

% \subsubsection{Projected Finite-Population LATE}
All LATE estimators considered so far presuppose evenly spaced sentinel points, which are then given weights.
Alternatively, we can project onto the border the treatment and control units that are within a pre-chosen distance \(\buffer\) of the border, and use these projected unit locations without weights (see \autorefexternal{fig:mississippi_projection_methods}\ofsupp{} for an illustration).
For any point \(\svec\), we use the notation \(\proj_{\border}\del{\svec}\) to give the coordinates of the point on the border \(\border\) that is closest to \(\svec\) (assuming uniqueness), and \(\dist_{\border}\del{\svec}\) for the distance between the point and the border.
Let \(\vicinity{\svec} = \allowbreak \Ind\cbr{\dist_{\border}\del{\svec} \le \buffer} \) indicate inclusion in the border vicinity.
The projected finite-population \(\tauproj\) is then the uniformly weighted mean applied with the projected unit locations instead of the evenly spaced sentinels.
We can therefore modify \autoref{eq:unifavg}, replacing the cliff height mean vector \(\muvec_{\sentinels \mid Y}\)
and covariance matrix \(\Sigma_{\sentinels \mid Y}\)
with their equivalents obtained at the projected unit locations,
to obtain the posterior mean and covariance of \(\tauproj\):
\begin{equation}\begin{split}
    &\tauproj \mid \Yvec, \hyperparam \sim \normal\del{\mu_{\tauproj \mid Y}, \Sigma_{\tauproj \mid Y}}\,\text{, with} \\
    &\mu_{\tauproj \mid Y} = 
    \sum_{i=1}^{n}
    \vicinity{\svec_i}
    \E\sbr{
        \tau\del{
            \proj_{\border}\del{\svec_i}
        }
        \mid \Yvec, \hyperparam
    } 
    \ 
    \Big/
    \ 
        \sum_{i=1}^{n}
        \vicinity{\svec_i}
    \,\text{, and}\\
    &\Sigma_{\tauproj \mid Y} = 
    \frac{
        \sum_{i=1}^{n} 
        \sum_{j=1}^{n} 
        \vicinity{\svec_i}
        \vicinity{\svec_j}
        \cov\sbr{
            \tau\del{
                \proj_{\border}\del{\svec_i}
            },
            \tau\del{
                \proj_{\border}\del{\svec_j}
            }
            \mid \Yvec, \hyperparam
        }
    }{
        \del{
            \sum_{i=1}^{n}
            \vicinity{\svec_i}
        }^2
    }
    \,.
\end{split}
\label{eq:tauproj}
\end{equation}
The posterior expectations and covariances in \autoref{eq:tauproj} are easily derived and computed analogously to the procedure of \autoref{sec:inference}.
Note that \(\tauproj\) is in the class of weighted mean estimands \autoref{eq:weighted_estimand},
with weight function \(\weightb(\sentinel) = \sum_{i=1}^{n} \vicinity{\svec_i} \delta\del{ \sentinel - \proj_\border(\svec_i)}\), where \(\delta\) is the Dirac delta function.

The resulting estimator has desirable properties: densely populated regions receive proportionately more projected units, but wigglier segments of the border do not.
While it lacks the information efficiency of the inverse-variance estimator,
the projected estimand is easier to understand and interpret,
and may feel more familiar to practitioners used to finite-population inference.
The averaging is over the observed units in the vicinity of the border, after they have been moved to the nearest point on the border.

In our experience, the choice of \(\buffer\) does not have a large effect on the estimate yielded by \autoref{eq:tauproj}.
A reasonable heuristic is to set \(\buffer\) to a small multiple of the Gaussian process lengthscale \(\ell\).
It should be noted that this choice only affects the location and density of projected units on the border; the \(\tauproj\) estimator assigns non-zero unit weights \autoref{eq:unit_weights} to all units, whether or not they fall within \(\buffer\) of the border.

\subsubsection{Summary}
\label{sec:summary}

The properties of the four LATE definitions proposed in this paper, and two additional choices presented \autorefexternal{sec:additional_late}\ofsupp{}, are summarized in \autoref{table:estimator_properties}.
In most applications, we recommend the use of the finite population or inverse-variance-weighted estimators, to prevent the undesirable influence of border topology.
The projected finite population method is simplest to understand and interpret in the tradition of finite population estimators, and unlike the density weighted LATE \(\taurho\) it does not require estimating population density.
Meanwhile, the inverse-variance estimator is the most efficient (lowest posterior variance) weighted mean estimator,
and sidesteps the choice of a distance cutoff for projected units.

\begin{table}[tbp]
    \centering
    \bgroup
    \def\arraystretch{1.1}%  1 is the default, change whatever you need
    \begin{tabular}{llllll}
        \hline
        Notation   & Description           & \(\border\) Topology & Sentinels & Principle & Variance \\
        \hline
        \(\unifavg\) & Uniform               & Sensitive & Equispaced      & Geometry    & High     \\
        \(\taurho\)  & Density-weighted      & Sensitive & Equispaced      & Population  & Low      \\
        \(\invvar\)  & Inverse-var. weighted & Robust    & Equispaced      & Information & Lowest   \\
        \(\tauproj\) & Projected finite pop. & Robust    & Projected       & Finite pop. & Low      \\
        \(\taugeo\)  & Proj. land            & Robust    & Proj. Grid  & Geography   & High     \\
        \(\taupop\)  & Proj. superpop.       & Robust    & Proj. Grid  & Population  & Low \\
        \hline
    \end{tabular}
    \egroup
    \caption{
    \label{table:estimator_properties}
    Summary of local average treatment effect estimator and estimand properties.}
\end{table}

\subsection{Testing for Non-Zero Effect}
\label{sec:hypothesis_testing}
Once we have obtained the ``cliff height'' estimate \autoref{eq:postvar2gp} and estimated a LATE, we might also naturally wonder whether we can claim to have detected a significant treatment effect at the border.
In the hypothesis testing framework, we distinguish two possible choices of null hypotheses:
the sharp null specifies that the treatment effect is zero everywhere along the border,
\(\tau(\sentinel)=0\:\forall\,\sentinel \in \border\), 
whereas the weak null only requires the LATE to be zero.
We focus on a test of the weak null hypothesis here, but also provide two tests of the sharp null hypothesis based on the marginal likelihood and a chi-squared statistic in \autorefexternal{sec:alternate_tests}\ofsupp{}.
We found through simulations and in our applied example that the test presented in this paper has superior power and robustness to model misspecification, and therefore recommend its use.

As we saw in \autoref{sec:ate}, the LATE estimand can be defined in multiple ways.
If we choose the inverse-variance weighted mean, then \(\invvar\) has posterior given by \autoref{eq:invvar}.
While the posterior is a Bayesian object, we can use it heuristically to derive a pseudo-\(p\)-value
\(
% \begin{equation}
    % \begin{split}
        % & Z_0 \sim \normal\del{0, \Sigma_{\invvar \mid Y}} \,, \\
        % & 
    \tilde{p}^{\mathrm{INV}} 
        % = \prob\del{ 
            % \abs{Z_0} > 
            % \abs{
                % \mu_{
                    % \invvar \mid Y
                % }
            % } 
        % }
        = \allowbreak 2\Phi ( % \del{-
            | %\abs{
                \mu_{
                    \invvar \mid Y
                }
            | %}
            \,/\,
            \allowbreak
            \sqrt{
                \Sigma_{\invvar \mid Y}
            }
    ) %}
% \end{split}
% \end{equation}
\).
However, this pseudo-\(p\)-value obtained from the Bayesian posterior may not have good frequentist properties.
In particular, there is no guarantee that under the null hypothesis, \(\tilde{p}^{\mathrm{INV}}\) is below 0.05 less than 5\% of the time.

To turn it into a valid frequentist test, it can be calibrated using a parametric bootstrap under the null.
We specify a parametric null model \(\modnull\)
as a single Gaussian process spanning the control and treatment regions,
with the same kernel and hyperparameters values obtained through the procedure of \autoref{sec:inference}.
Under \(\modnull\), the expected outcomes surface is smooth and continuous at the border,
and therefore accords with both the sharp and weak null hypotheses.
We now choose the posterior mean of the inverse-variance LATE \(\mu_{\invvar \mid Y}\) as a test statistic.
For \(b=1,\dotsc,B\) iterations, we draw \(\Yvec^{(b)}\) from \(\modnull\),
using the same spatial locations as the original data,
and compute \(\mu_{\invvar \mid Y^{(b)}}\) according to \autoref{eq:invvar} applied to the simulated data rather than the true data.
The proportion of \(\mu_{\invvar \mid Y^{(b)}}\) with absolute value greater than the observed \(\mu_{\invvar \mid Y^{obs}}\) estimates the \(p\)-value:
\begin{equation}
    p^{\mathrm{INV}} = \prob\del{ \abs{\mu_{\invvar \mid Y}} \ge \abs{\mu_{\invvar \mid Y^{obs}}} \mid \modnull} 
    \approx \frac{1}{B} 
    \sum_{b=1}^B 
        \Ind\cbr{
            \abs{
                \mu_{\invvar \mid Y^{(b)}}
            } 
            \ge  
            \abs{
                \mu_{\invvar \mid Y^{obs}}
            } 
        }
    \,.
\end{equation}
Computationally, because the hyperparameters and locations of the units are held constant during the bootstrap, we can reuse the Cholesky decomposition of the covariance matrix, allowing the test to be performed in seconds even with hundreds of units and thousands of bootstrap samples.

The calibration can also be achieved analytically, since \(\mu_{\invvar \mid Y}\) is normally distributed under the null hypothesis.
We derive the analytical calibration of hypothesis tests based on any LATE estimand in \autoref{sec:calibration}.
Note that the \(p\)-value for this test is derived under the parametric null model \(\modnull\), which accords with both the sharp null and weak null hypotheses, but is not the only possible model that satisfies the weak null.
The calibrated inverse-variance test “targets” the weak null hypothesis in the sense that the test statistic is an estimate of the LATE, and thus the test is sensitive to deviations of the LATE from zero, rather than its \(p\)-value being derived directly under the weak null (such as the classical \(t\)-test).

% \label{eq:calib_test}

\subsubsection{Placebo Tests}
\label{sec:placebo}
Gaussian process models are almost always misspecified.
We do not believe that the Gaussian process with stationary Mat\'ern kernel is the true data-generating process, although we hope that the model is sufficiently flexible to represent reality well.
Under misspecification, we should be skeptical of results that rely on the truth of the model specification.
We therefore encourage practitioners to probe the validity of the hypothesis test by running a ``placebo'' test.
A placebo test repeatedly applies the hypothesis test on data that are known to have zero treatment effect (a ``placebo''),
in order to verify that the returned \(p\)-values are uniformly distributed.
In our spatial setting, we use the treatment and control regions separately as placebo groups.
Within each placebo group, we repeatedly draw an arbitrary geographical border, creating new treatment and control groups.
Here we drew lines that split the placebo units in half at a sequence of angles \(1\degree,2\degree,3\degree,\dotsc,180\degree\) counter-clockwise from horizontal, each positioned so that half of the units fall on either side of the line in order to maximize power.
Because the borders are drawn arbitrarily, without reference to the outcomes, we should not expect to observe a discontinuous jump in outcomes.
We apply the calibrated inverse-variance test procedure described above to the data arbitrarily divided by each placebo border, and hope to obtain a roughly uniform distribution of \(p\)-values.
The placebo \(p\)-values are highly correlated, resulting in a small effective sample size, but this procedure nonetheless allows us to visually verify that the \(p\)-values are not blatantly biased.
\citet{gibbons2013valuing} perform a falsification test that is similar in spirit to our procedure. They shift the locations of the housing transactions 10~km North and East, and show that their matching method, unlike OLS, no longer yields a significant estimate of the effect of school quality on house prices.

\section{Valuing NYC School Districts}
\label{sec:NYC_example}

We now use our methodology to study the effect
of school districts on house prices in New~York City.
The city publishes information pertaining to property sales within the city in the last 12 months on a rolling basis,
available at \url{https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page},
which we downloaded on September~15, 2016.
The dataset includes columns for the sale price, building class, and the address of the property.
Public schools in the city are all part of the City School District of the City of New York, but the city-wide district is itself divided into 32 sub-districts.
It is a common belief that school districts have an impact on real estate price, as parents are willing to pay more to live in districts with better schools.
We therefore ask whether we can measure a discontinuous jump in house prices across the borders separating school districts.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{sales_map.png}
    \caption{\label{fig:sales_map}Map of property sales in New York City. Each dot is a sale, and its color indicates the price per square foot. White crosses indicate sales of properties with missing square footage, which are therefore excluded from the analysis. School district boundaries are shown, and each district is labeled by its number.}
\end{figure}

In order to model the property sale prices, we first need to obtain their locations.
We geocode the address of each sale by merging the sales with NYC's Pluto database, which contains X and Y coordinates for each house, identified by its borough, zip code, block and lot.
These coordinates are given in the EPSG:2263 projection, which we also adopt.
For addresses that do not find a match in Pluto, we use Google's geocoding API to obtain a latitude and longitude, which we then project onto EPSG:2263.

We then filter the 56,815 sales, by removing
36,448 sales outside of the family homes building class categories (one, two, and three family dwellings),
4 remaining sales missing the square footage information,
% \item without a reported sale price,
% \item without a location due to failed geocoding,
% \item smaller than 100 sq ft, and 
and 785 remaining sales with outlier log price per square foot less than 3 or more than 8.
We exclude condos and coops because only very few sales report square footage alongside the price.
The resulting dataset of 19,578 sales is displayed in \autoref{fig:sales_map}.
The 27,394 residential properties with missing square footage information are also shown;
these are almost all coops and condos, which explains the clustering of missing data in areas of higher density.

\subsection{Model for Property Prices}

Our application relates to the economics literature on valuing school quality \citep{black2011housing}, 
based on the hedonic valuation model \citep{rosen1974hedonic,sheppard1999hedonic} which typically takes the form of a linear model for the log of the sale price \(p\) of a property at location \(\svec\) \citep[see for example][]{gibbons2013valuing}:
\begin{equation}
    p = s(\svec)\beta + x(\svec)\gamma + g(\svec) + \epsilon
    \,,
    \label{eq:hedonic}
\end{equation}
where \(s(\svec)\) is the expected quality of the schools that residents near \(\svec\) can access,
\(x\) is a set of observed property and neighborhood covariates, \(g(\svec)\) captures spatially correlated unobserved covariates, and \(\epsilon\) represents unobserved characteristics of the property and errors that are independent of \(x\) and \(\svec\).
``School quality'' is variously defined and estimated \citep{black2011housing}: average standardized test scores, survey responses, funding levels, pass rates, publicly, etc.

Most research focuses on estimating \(\beta\), the effect of a unit of school quality on the log-price, while addressing confounding due to \(g(\svec)\).
Imagine moving a property sale an infinitesimal distance from one side of the border to the other; 
the difference in prices is then:
\begin{equation}
    \Delta(p) = \Delta(s(\svec) \beta) + \Delta(x(\svec)\gamma)+ \Delta{g(\svec)} + \Delta{\epsilon}
    \,.
    \label{eq:hedonic_diff}
\end{equation}
The last three terms on the right-hand-side equal zero: the property has not changed, and \(x\) and \(g\) are assumed smooth and continuous, so only the change in attendance districts can explain the change in price.
The GeoRDD uses this idea to identify \(\Delta(s(\svec) \beta)\), the jump in price attributable to the difference between school districts.
However, to estimate \(\beta\), the hedonic model requires the further assumptions that school quality is well-defined and measured, and that its effect on log-prices is linear and constant.
In this paper, we avoid these assumptions by seeking to directly estimate the jump in prices at the border, without attempting to attribute the jump to measures of school quality.

In our application, the outcome of interest is price per square foot of a property sale.
As is commonly done in analyses of real estate prices, we take its logarithm to reduce the skew in the outcomes.
The complete model is then a Gaussian process within each district (indexed by \(j=1,\dotsc,J_\district\)) over the spatial covariates \(\svec\), super-imposed with a linear regression on the property covariates (which are \(L_\building\) building categories encoded as dummy variables):
\begin{equation}
    \begin{aligned}
        & Y_i = % \log\del{ \frac{\saleprice_i}{\sqft_i}} =
            m_{\district\sbr{i}} + \gamma_{\building\sbr{i}}
            + f_{\district\sbr{i}}(\svec_i) + \epsilon_i 
            \,,\quad
             \epsilon_i \overset{\indep}{\sim} \normal\del{0, \sigman^2} 
            \,,
            \phantom{\hspace{-10cm}} % for alignment
            \\
        & \gamma_{l} \sim \normal\del{0, \sigmabeta^2}
            \,,
            &&
            \text{for }
            l=1,\dotsc,L_\building \,, \\
        & m_{j} \sim \normal\del{0, \sigmamu^2}
        \,,\ 
        f_j \sim \GP\del{0, k(\svec, \svec')}
            \,,
            &&
            \text{for }
            j=1,\dotsc,J_\district
        \,.
        % k(\svec, \svec') &= \sigmaf^2 \exp\cbr{ - \frac{(\svec-\svec')\trans(\svec-\svec')}{2\ell^2}} \,,
    \end{aligned}
    \label{eq:nyc_model}
\end{equation}
where \(k\) is the Mat\'ern covariance function as in \autoref{eq:spec2gp}.

A visual inspection of the house sales map in \autoref{fig:sales_map} drew our attention towards the border between districts 19 and 27, which we arbitrarily designate as ``treatment'' and ``control'' respectively.
Importantly, the border between the two districts is also part of the border between Brooklyn and Queens.
This is an instance of what \citet{keele_titiunik_2015} term ``compound treatments,'' a frequent concern in GeoRDDs.
Therefore, we are \emph{measuring} a discontinuity in the house prices at the border,
but attributing the discontinuity to a particular cause (school district or borough) is not directly supported by the data.

Another concern is units sorting around the border, which would violate the identification assumptions for GeoRDDs.
We take the view that the unit of analysis here is the tract of land on which houses are built, rather than the residents themselves.
If a district becomes more attractive, people may move to it, whereas land does not move but its price adjusts.
A sale gives a snapshot of the price of the land, made more accurate by correcting in our model \autoref{eq:nyc_model} for covariates that pertain to the building rather than land.
% Note that the limited covariates provided by the data cannot fully capture the value of the building.
% For example, the wealthier residents who inhabit the more desirable school districts may also have more funds available to maintain and enhance their home, which will drive up the property's resale value.
% Since it is not captured by the available covariates, this added value is folded into the treatment effect by our analysis.

% \begin{figure}[tb]
    % \centering
    % \includegraphics[width=0.5\textwidth]{sales_histogram_19-27.pdf}
    % \caption{\label{fig:NYC_histogram}Histogram of log sale prices per square foot of houses sold in NYC school districts 19 and 27.}
% \end{figure}

% The histogram in \autoref{fig:NYC_histogram} of log prices per square foot for sales in both districts shows that marginally the house prices are very different.
% Our goal is to establish whether this difference is measurable at the border, and not merely an underlying trend that spans both districts.

\subsection{Cliff Height Estimator}

We fit the hyperparameters \(\sigmabeta\), \(\sigmaf\), \(\ell\) and \(\sigman\) by optimizing the marginal log-likelihood of the data within neighboring school districts 18, 19, 23, 24, 25, 26, 27, 28, and 29.
We hold \(\sigmamu\) fixed to 20 to give the district means \(m_j\) a weak prior.
% The fitted hyperparameters were \(\widehat{\sigman}=0.40\), \(\widehat{\sigmaf}=0.20\), \(\widehat{\sigmabeta}=0.15\), and \(\widehat{\ell}=1.4~\text{km}\). % Squared Exponential kernel
The fitted hyperparameters were \(\widehat{\sigman}=0.40\), \(\widehat{\sigmaf}=0.26\), \(\widehat{\sigmabeta}=0.14\), and \(\widehat{\ell}=5.0~\text{km}\). % Matern kernel

We seek to estimate the treatment effect function \(\tau\) on the border between the two districts.
We could proceed by computing the cliff height estimator with covariates \autoref{eq:cliff_with_covariates}.
But to simplify the analysis as discussed in \autorefexternal{sec:covariates}\ofsupp{}, we can instead obtain the posterior means of the \(\gamma\) coefficients, and extract the residuals \(\Yvec{}-\Xmat \hat{\gammavec}\).
We then treat the residuals as the observed outcomes in a GeoRDD analysis with no non-spatial covariates.
In this example, we find that the posterior variance of \(\gammavec\) is low, and therefore the two approaches yield very similar results, but conditioning on the estimate of \(\gammavec\) is computationally convenient.

\begin{figure}[tb]
    \centering
    \includegraphics[height=0.35\textheight]{NYC_cliff_face.png}
    \caption{\label{fig:NYC_cliff_height}
        (a)
        Cliff height estimator \autoref{eq:postvar2gp} for the school district effect on house prices per square foot between district 27 and district 19, with 95\% credible envelope.
        The left \(y\)-axis shows the difference in log prices per square foot; positive values mean prices are higher in district 19.
        The right \(y\)-axis shows the corresponding ratio of the price of a house in district 19 over its price in district 27.
        A few draws from the posterior are shown in lighter color to show the posterior correlations between sentinels.
        Note the decorrelation from sentinels 6 to 7, and 19 to 20, where the border crosses the water between sparsely populated islands in Jamaica Bay and then onto Long Island.
        (b)
        A map of sentinel locations, evenly spaced along the border between school districts 27 and 19.
        The southernmost sentinel (shown as a blue circle in both plots) has index 1, while the northernmost sentinel (shown in yellow) has index 100.
    }
\end{figure}

Following the inference procedure outlined in \autoref{sec:gpmodel}, we obtain the posterior distribution of the cliff height \(\tau(\sentinels)\) obtained at \(R=100\) sentinel locations evenly spaced along the border.
The cliff height is shown in \autoref{fig:NYC_cliff_height}, and shows that \(\tau\) is estimated as negative everywhere along the border, which corresponds to higher property prices in district 27.
However, the credible envelope is fairly wide, especially in the southern section of the border, so we cannot visually rule out the null hypothesis that \(\tau=0\).

\subsection{Average Log-Price Increase}
The cliff height \autoref{fig:NYC_cliff_height} shows a negative treatment effect everywhere along the border, which can be averaged by the estimators we developed in \autoref{sec:ate}.
Our two recommended estimators, based on inverse-variance weighting and finite-population projection of units within \(\buffer=\ell\) of the border, yield LATE estimates of \(-0.20\) and \(-0.17\) respectively, which corresponds to a roughly 20\% increase in property prices going from district 19 to district 27.
By contrast, treating each district and building class as a fixed effect in an OLS model yields a treatment effect estimate (the difference between the district 19 and 27 coefficients) of -0.12. 
This smaller estimate could be explained by an overall East to West positive spatial trend in prices, visible between districts 29 and 15 in \autoref{fig:sales_map}, which would confound the OLS estimate of the treatment effect.
All LATE estimators from \autoref{sec:ate} applied to this setting are shown in \autoref{table:NYC_ate}.
In this example the different estimators yield similar answers, as the border is fairly straight and short relative to the fitted lengthscale.


\begin{table}
    \centering
    \begin{tabular}{lrrr}
        \hline
        & \multicolumn{3}{c}{Posterior} \\
        Estimand & Mean & Standard Dev. & Tail Prob. \\
        \hline
          % \(\unifavg\)                 & -0.17 & 0.08 & 1.98\% \\
          % \(\taurho\)                  & -0.19 & 0.06 & 0.04\% \\
          % \(\invvar\)                  & -0.19 & 0.06 & 0.03\% \\
          % \(\tauproj~(\buffer=2\ell)\) & -0.18 & 0.06 & 0.13\% \\
          % \(\taugeo~(\buffer=2\ell)\)  & -0.16 & 0.09 & 3.80\% \\
          % \(\taupop~(\buffer=2\ell)\)  & -0.18 & 0.06 & 0.15\% \\
          \(\unifavg\) & -0.16 & 0.09 & 4.52\% \\
          \(\taurho\)  & -0.19 & 0.06 & 0.14\% \\
          \(\invvar\)  & -0.20 & 0.06 & 0.09\% \\
          \(\tauproj\) & -0.17 & 0.08 & 1.98\% \\
          \(\taugeo\)  & -0.14 & 0.13 & 15.37\% \\
          \(\taupop\)  & -0.17 & 0.08 & 1.34\% \\
        \hline
    \end{tabular}
    \caption{
    \label{table:NYC_ate}
Average difference in log price per square foot between school districts 19 and 27. For each LATE estimand, we show the mean and standard deviation of its posterior distribution, and the tail probability \(\prob(\tau > 0 \mid \Yvec, \hat{\gamma}, \hyperparam)\). 
Negative LATEs correspond to district 27 being more expensive.
}
\end{table}

\subsection{Significant Difference in Price?}
The estimated inverse-variance weighted mean treatment effect is suggestive of a significant treatment effect.
But the posterior tail probability cannot be interpreted as a \(p\)-value.
For this, we turn to the test developed in \autoref{sec:hypothesis_testing}, which yields a \(p\)-value of \(p^{\mathrm{INV}}=0.005\), thus rejecting the null hypothesis that there is no difference in house prices at the border between districts 19 and 27.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth,height=0.3\textheight,keepaspectratio]{placebo_invvar.pdf}
    \caption{
    \label{fig:placebo_invvar} Placebo tests for calibrated inverse-variance test of difference in house prices at the border between NYC school districts 19 and 27. 
    (a) the placebo \(p\)-value as a function of the border angle;
    (b) histogram of the placebo \(p\)-values, 
    with the black vertical line indicating the uniform distribution.
    }
\end{figure}

To assess the validity of the test, we apply the placebo tests devised in \autoref{sec:placebo},
the results of which are shown in \autoref{fig:placebo_invvar}.
Within each district, we split the data in half by a line at angles \(1\degree\), \(2\degree\), \(\dotsc\), \(180\degree\).
Because these lines were drawn arbitrarily, we do not expect a discontinuous treatment effect between the two halves, and so we hope to see a uniform distribution of placebo \(p\)-values.
However, these tests will be highly correlated---there is in fact noticeable autocorrelation in the graph of the placebo \(p\)-value as a function of angle---and so the low effective sample size could lead to some apparent departures from uniformity.
Nonetheless, we do not observe a flagrant bias in \autoref{fig:placebo_invvar}(b), which therefore does not discredit the calibrated inverse-variance test, and confirms the significance of the difference in price at the border between the two districts.

\section{Discussion}

Geographic regression discontinuity designs (GeoRDDs) arise when a treatment is assigned to one region, but not to another adjacent region.
% For outcomes that vary spatially, a direct comparison of mean outcomes between \(\yt\) and \(\yc\), such as a \(t\)-test, is an invalid estimator of the treatment effect, as it is confounded by the spatial covariates.
Under smoothness assumptions, units adjacent to the border are comparable, and form a natural experiment.
The same idea underpins causal interpretations of one-dimensional regression discontinuity designs (1D RDDs), where a single ``forcing'' variable controls the treatment assignment instead of a border separating two geographical regions.
We use this similarity to motivate a framework for the analysis of GeoRDDs, which proceeds in three steps:
% One-dimensional methods can be abstracted to three steps: (1) fit a smooth function on either side of the threshold; (2) extrapolate the functions to the threshold; and (3) take the difference of the two extrapolations to estimate the treatment effect at the threshold.
\begin{flatlist} 
\item fit a smooth surface on either side of the border,
\item extrapolate the surfaces to the border, and 
\item take the difference of the two extrapolations to estimate the treatment effect along the border.
\end{flatlist}

% Previous research has focused on extending methods developed for 1D RDDs to GeoRDDs.
% In applied settings, some have used the signed distance from the border as the forcing variable in a 1D RDD, but the resulting estimator is spatially confounded.
In this paper, we emphasize the importance of the spatial aspect of the design, and therefore draw from the spatial statistics literature, which brings a rich set of tools designed to model and exploit spatial correlations.
We used Gaussian process regression (GPR), known as kriging in the spatial statistics literature, to fit the smooth surfaces to the outcomes in step (1) of our framework.
Our approach yields a multivariate normal posterior distribution of the treatment effect for a collection of ``sentinel'' locations along the border.

Averaging the treatment effect along the border turns out to have surprising pitfalls.
Simply integrating the treatment effect uniformly along the border yields an estimand that is inefficient and undesirably sensitive to the topology of the border.
More sophisticated estimands, summarized in \autoref{table:estimator_properties}, are robust to this effect, and use the information available in the data more efficiently.

% We recommend the use of the calibrated inverse-variance test, derived from the posterior distribution of the inverse-variance LATE estimator.
% It has generally high statistical power and behaved well in placebo tests in the NYC application.
To test against the null hypothesis of zero treatment effect along the border, we develop a test based on the posterior distribution of the LATE.
We use the inverse-variance weighted LATE to attain high power, but the other LATE estimates of \autoref{sec:ate} could be used similarly.
To ensure good frequentist properties we “calibrate” the test, obtaining its distribution under the null model, either using a parametric bootstrap or analytically.

We applied our method to a publicly available dataset of one year of New York City property sales, to examine whether school district cause difference in property prices.
While this relates to the literature on valuing school quality through house prices, we focus on inferring the discontinuity in house prices at the border, without attempting to attribute it to a difference in various metrics of school quality.
Focusing on a single border, we estimated a roughly 20\% average increase in house prices per square foot when crossing the border from district 19 to district 27.
However, the border between these two districts is also the border between the NYC boroughs of Brooklyn and Queens, so we cannot attribute this difference to the causal effect of the school districts.
In the Supplementary Materials, we extend the GeoRDD analysis to other pairs of adjacent school districts, and find significant effects between many of these pairs.
However, in some of these cases, physical barriers like parks, commercial zones, railways, and major roads can separate neighborhoods. 
This keeps data away from the borders, breaks the stationarity assumption of the spatial model, and increases the extent of extrapolation performed by the model, which casts doubt on the legitimacy of the estimated treatment effects.
% Missing data from condo sales which do not report square footage can also distort estimated effects.
% Overall, it seems that school district borders in Brooklyn and Queens are often accompanied by a discontinuity in house prices, but the causal attribution of this difference to the reputation of the school districts can be questionable.

The main limitation of our approach to GeoRDDs is the reliance on modeling assumptions.
We modeled the response surfaces as two independent Gaussian processes, with \iid{} normal noise for each observation.
As is common in spatial statistics, we use Gaussian process regression as a non-parametric smoothing device that flexibly captures spatial correlations, but do not claim that our model is a true representation of the stochastic mechanism generating the data.
We believe care must therefore be taken not to lean heavily on modeling assumptions.
In particular, we recommend that hypothesis tests always be accompanied by placebo tests:
by applying the same procedure with arbitrary borders where no treatment was applied, we can verify that the test behaves appropriately under the null hypothesis, despite any potential model misspecification.
We also assumed a stationary covariance structure, with hyperparameters equal in the treatment and control regions, and we chose the Mat\'ern covariance kernel.
The assumption of equal covariance parameters in the two areas can be relaxed, by separately tuning the parameters within each area.

Because of the need to extrapolate the fitted processes a short distance to the border, our GeoRDD method may be vulnerable to the limitations of Gaussian processes when extrapolating.
The distinction between interpolation and extrapolation of spatial models is explored in some depth in \citet{stein2012interpolation}.
We expect that methodological advances that improve the extrapolating behavior of Gaussian processes would also improve the robustness of our method.
For example, \citet{wilson2013gaussian} develop spectral mixture (SM) covariance kernels with good extrapolating behavior, which could be applied beneficially to GeoRDDs.
However, SM kernels are motivated by time series with some periodic or oscillatory behavior, which is more unusual in spatial applications, and may therefore not be as well-suited for use with GeoRDDs.

The use of GPR to analyze GeoRDDs gives flexibility and extensibility to the method.
This presents many opportunities for future research, inspired by the past and future development of methods in spatial statistics and machine learning that are based on Gaussian processes.
% In spatial statistics, kriging has been used as the foundation for a plethora of spatial models, which may be adapted for the purposes of analyzing GeoRDDs.
\citet{banerjee2014hierarchical} provides a good introduction to the richness of the spatial statistics field.
For example, if the outcomes are binary, proportions, or counts, then binomial or Poisson likelihoods could be substituted instead of the normal likelihood used in this paper.

Furthermore, in some applications, it may be of substantive interest to know whether the treatment effect is constant (homogeneous) or variable (heterogeneous).
Hypothesis tests targeting the homogeneity of the treatment effect along the border would be an interesting possible extension of our framework.

The framework and techniques of this paper could also be extended to spatio-temporal settings.
If the treatment is only applied to the treatment region after a time \(t^*\), one could envision a three-dimensional RDD consisting of the geographical border in the spatial dimensions, and a straight line through \(t^*\) in the temporal dimension.
The Gaussian process model would need to be augmented with a temporal component, for example with an anisotropic squared exponential covariance function.
We leave spatio-temporal RDDs using Gaussian process models to future research.
